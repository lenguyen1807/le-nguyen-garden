---
{"dg-publish":true,"permalink":"/notes/curve-fitting-revisited/","noteIcon":"üìù","created":"2024-08-02T10:46:38.338+07:00","updated":"2024-08-02T10:47:44.750+07:00"}
---


·ªû ph·∫ßn [[Notes/Polynomial Curve Fitting\|Polynomial Curve Fitting]], ta ƒë√£ bi·∫øt c√°ch ƒë·ªÉ x·∫•p x·ªâ m·ªôt h√†m ƒëa th·ª©c $f(x, \mathbf{w}) = \sum_{n=0}^M x^n w_{n}$ b·∫±ng c√°ch t·ªëi thi·ªÉu h√†m m·∫•t m√°t
$$
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N [f(x_{n}, \mathbf{w}) - y_{n}]^2
$$
·ªû ph·∫ßn n√†y, ta c≈©ng s·∫Ω l√†m nh∆∞ v·∫≠y nh∆∞ng v·ªõi g√≥c nh√¨n x√°c su·∫•t Bayes. V·∫´n d√πng l·∫°i v√≠ d·ª• c≈©, gi·∫£ s·ª≠ LN thu th·∫≠p ƒë∆∞·ª£c m·ªôt b·ªô d·ªØ li·ªáu $\mathbf{x} = \{x_{1}, x_{2}, \dots, x_{N}\}^T$ sau $N$ l·∫ßn mua, v·ªõi m·ªói s·ªë ti·ªÅn $x_{i}$, LN mua ƒë∆∞·ª£c s·ªë b√°nh x√®o $y_{i}$, v·∫≠y $\mathbf{y} = \{ y_{1}, y_{2}, \dots, y_{N}\}^T$ v√† LN gi·∫£ s·ª≠ d·ªØ li·ªáu tu√¢n theo h√†m ƒëa th·ª©c:
$$
f(x, \mathbf{w}) = \sum_{n=0}^M x^{n} w_{n}
$$
Nh∆∞ng s·∫Ω kh√°c v·ªõi v√≠ d·ª• ban ƒë·∫ßu ·ªü ch·ªó n√†y, ta gi·∫£ s·ª≠ v·ªõi m·ªói s·ªë b√°nh x√®o $x$, s·ªë ti·ªÅn $y$ tu√¢n theo m·ªôt ph√¢n ph·ªëi chu·∫©n v·ªõi trung b√¨nh l√† gi√° tr·ªã c·ªßa ƒëa th·ª©c $f(x, \mathbf{w})$ v√† ph∆∞∆°ng sai l√† $\beta^{-1}$ (nh∆∞ ·ªü ph·∫ßn [[Notes/Gaussian Distribution\|Gaussian Distribution]], $\beta$ l√† ƒë·ªô ch√≠nh x√°c c·ªßa ph√¢n ph·ªëi v√† $\beta = 1 / \sigma^2$). V·∫≠y ta c√≥ ph√¢n ph·ªëi c·ªßa $y$ v·ªõi tham s·ªë l√† $x, \mathbf{w}, \beta$ l√†:
$$
p(y \mid x, \mathbf{w}, \beta) = \mathcal{N}(y \mid \mu = f(x, \mathbf{w}), \sigma^2 = \beta^{-1})
$$
>[!note]+
>M√¨nh kh√¥ng bi·∫øt t·∫°i sao t√°c gi·∫£ l·∫°i ch·ªçn trung b√¨nh v√† ph∆∞∆°ng sai c√≥ gi√° tr·ªã nh∆∞ n√†y ü•≤. Trung b√¨nh th√¨ m√¨nh ch·ªãu, nh∆∞ng vi·ªác ch·ªçn ƒë·ªô ch√≠nh x√°c ${} \beta^{-1} {}$ thay cho ph∆∞∆°ng sai $\sigma^2$ s·∫Ω c√≥ l·ª£i √≠ch nh∆∞ n√†y.
>$$
\begin{align}
>\mathcal{N}(y \mid \mu, \beta^{-1}) &= \frac{1}{\sqrt{ 2\pi \beta^{-1} }} \exp\left( \frac{1}{2\beta^{-1}} (y - \mu)^2 \right) \\
&= \frac{\sqrt{ \beta }}{\sqrt{ 2\pi }} \exp\left( \frac{\beta}{2}(y -\mu)^2 \right)
\end{align}
>$$
>C√≥ th·ªÉ th·∫•y c√°c tham s·ªë ƒë·ªÅu c·ªßa ph√¢n ph·ªëi chu·∫©n ƒë·ªÅu n·∫±m ·ªü tr√™n t·ª≠, ƒëi·ªÅu n√†y s·∫Ω gi√∫p cho vi·ªác t√≠nh to√°n d·ªÖ d√†ng h∆°n.

Gi·ªù LN s·∫Ω s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu ƒë√£ c√≥ $\{\mathbf{x}, \mathbf{y}\}$ ƒë·ªÉ t√¨m gi√° tr·ªã $\mathbf{w}$ v√† $\beta$ sao cho $p(y \mid \mathbf{w}, \beta)$ l√† l·ªõn nh·∫•t b·∫±ng c√°ch d√πng maximum log likehood. Gi·∫£ s·ª≠ r·∫±ng b·ªô d·ªØ li·ªáu c·ªßa LN ƒë∆∞·ª£c l·∫•y ra m·ªôt c√°ch ƒë·ªôc l·∫≠p t·ª´ ph√¢n ph·ªëi chu·∫©n $\mathcal{N}(y \mid f(x, \mathbf{w}), \beta^{-1})$, ta c√≥:
$$
\begin{align}
\ln p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta) &= \ln \prod_{n=1}^N p(y_{n} \mid x_{n}, \mathbf{w}, \beta) \\ 
&= \sum_{n=1}^N \ln p(y_{n} \mid x_{n}, \mathbf{w}, \beta) \\
&= \sum_{n=1}^N \ln \mathcal{N}(y_{n} \mid f(x_{n}, \mathbf{w}), \beta^{-1})
\end{align}
$$
·ªû [[Notes/Maximum Log likelihood\|Maximum Log likelihood]], ta ƒë√£ ƒë∆∞a ra c√¥ng th·ª©c t·ªïng qu√°t cho $\sum_{x} \ln \mathcal{N}(x \mid \mu, \sigma^2)$, gi·ªù ch·ªâ c·∫ßn thay $x = y_{n}, \mu = f(x_{n}, \mathbf{w})$ v√† $\sigma^2 = \beta^{-1}$, ta ƒë∆∞·ª£c:
$$
\begin{align}
\sum_{n=1}^N \ln \mathcal{N}(y_{n} \mid f(x_{n}, \mathbf{w}), \beta^{-1}) &= \left[ -\frac{1}{2\beta^{-1}} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 \right] - \frac{N}{2}\ln 2\pi - \frac{N}{2} \ln \beta^{-1} \\
&= \left[ -\frac{\beta}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 \right] - \frac{N}{2} \ln 2\pi +\frac{N}{2} \ln \beta
\end{align}
$$
ƒê·∫ßu ti√™n ƒë·ªëi v·ªõi tham s·ªë $\mathbf{w}$, ta th·∫•y $-\frac{N}{2} \ln 2\pi$ v√† $\frac{N}{2} \ln \beta$ l√† c√°c gi√° tr·ªã kh√¥ng li√™n quan ƒë·∫øn $\mathbf{w}$, khi ƒë·∫°o h√†m b·∫±ng $0$ do ƒë√≥ ta c√≥ th·ªÉ b·ªè ƒëi hai gi√° tr·ªã ƒë√≥, ngo√†i ra gi√° tr·ªã $-\frac{\beta}{2}$ l√† m·ªôt h·∫±ng s·ªë, khi ƒë·∫°o h√†m v√† l·∫•y b·∫±ng $0$ ta kh√¥ng c·∫ßn x√©t ƒë·∫øn h·∫±ng s·ªë ƒë√≥, ta c√≥ th·ªÉ lo·∫°i b·ªè lu√¥n ƒëi $\beta$ v√† ch·ªâ gi·ªØ l·∫°i $-\frac{1}{2}$ (m·ª•c ƒë√≠ch ƒë·ªÉ cho gi·ªëng b√†i to√°n least square). V·∫≠y m·ª•c ƒë√≠ch c·ªßa ta l√† t√¨m $\mathbf{w}$ sao cho h√†m d∆∞·ªõi ƒë√¢y l√† l·ªõn nh·∫•t:
$$
-\frac{1}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2
$$
ƒê·ªÉ c·ª±c ƒë·∫°i h√†m log th√¨ ta c√≥ th·ªÉ c·ª±c ti·ªÉu h√†m log √¢m, v·∫≠y ta t√¨m $\mathbf{w}$ sao cho h√†m d∆∞·ªõi ƒë√¢y l√† nh·ªè nh·∫•t:
$$
\frac{1}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2
$$
C√≥ th·ªÉ th·∫•y, b√†i to√°n t√¨m $\mathbf{w}$ sao cho log likehoood c·ª±c ƒë·∫°i ƒë√£ ƒë∆∞a v·ªÅ th√†nh b√†i to√°n b√¨nh ph∆∞∆°ng nh·ªè nh·∫•t (least square) nh∆∞ ·ªü b√†i [[Notes/Polynomial Curve Fitting\|Polynomial Curve Fitting]]. ƒê·∫∑t gi√° tr·ªã t·ªëi ∆∞u $\mathbf{w}$ m√† ta t√¨m ƒë∆∞·ª£c l√† $\mathbf{w}_{ML}$.

C√≤n ƒë·ªëi v·ªõi tham s·ªë $\beta$, n·∫øu l·∫•y ƒë·∫°o h√†m c·ªßa log likelihood, ta ƒë∆∞·ª£c:
$$
-\frac{1}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 + \frac{N}{2} \frac{1}{\beta}
$$
V·∫≠y ƒë·ªÉ ph∆∞∆°ng tr√¨nh tr√™n b·∫±ng $0$, ta c√≥ gi√° tr·ªã t·ªëi ∆∞u $\beta_{ML}$ l√†:
$$
\frac{1}{\beta_{ML}} = \frac{1}{N} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}_{ML}))^2
$$
Do ƒë√£ c√≥ hai tham s·ªë $\mathbf{w}$ v√† $\beta$ c·∫ßn thi·∫øt l√† $\mathbf{w}_{ML}$ v√† $\beta_{ML}$ th√¥ng qua maximum likelihood, ta c√≥ ƒë∆∞·ª£c ph√¢n ph·ªëi:
$$
p(y \mid x, \mathbf{w}_{ML}, \beta_{ML}) = \mathcal{N}(y \mid f(x, \mathbf{w}_{ML}), \beta^{-1}_{ML})
$$
ta g·ªçi ph√¢n ph·ªëi tr√™n l√† **ph√¢n ph·ªëi d·ª± ƒëo√°n** (predictive distribution). Kh√°c v·ªõi ph·∫ßn [[Notes/Polynomial Curve Fitting\|Polynomial Curve Fitting]], ta kh√¥ng t√¨m m·ªôt gi√° tr·ªã $y$ c·ª• th·ªÉ v·ªõi $x$, m√† ta t√¨m ƒë∆∞·ª£c c·∫£ m·ªôt ph√¢n ph·ªëi c·ªßa $y$, khi thay m·ªôt gi√° tr·ªã $x_{0}$ b·∫•t k√¨ v√†o, ta t√¨m ƒë∆∞·ª£c ph√¢n ph·ªëi $p(y \mid x_{0}, \mathbf{w}_{ML}, \beta_{ML})$. 

V·∫≠y l√†m sao ƒë·ªÉ t√¨m gi√° tr·ªã $y_{0}$ n·∫øu thay th·∫ø gi√° tr·ªã $x_{0}$ cho c·∫£ m·ªôt ph√¢n ph·ªëi thay v√¨ m·ªôt gi√° tr·ªã, th√¥ng th∆∞·ªùng ng∆∞·ªùi ta s·∫Ω l·∫•y trung b√¨nh c·ªßa ph√¢n ph·ªëi $p(y \mid x_{0}, \mathbf{w}_{ML}, \beta_{ML})$, t·ª©c l√† $y_{0} = f(x_{0}, \mathbf{w}_{ML})$ (√† th√¨ ra ƒë·∫•y l√† l√Ω do t·∫°i sao ch·ªçn trung b√¨nh l√† ${} f(x, \mathbf{w})$ üíÄ). V·∫≠y c≈©ng quay ng∆∞·ª£c l·∫°i gi·ªëng v·ªõi ph·∫ßn [[Notes/Polynomial Curve Fitting\|Polynomial Curve Fitting]], th·∫ø nh∆∞ng vi·ªác c√≥ th√™m ph√¢n ph·ªëi d·ª± ƒëo√°n, ta c√≥ ƒë∆∞·ª£c nhi·ªÅu th·ª© ƒë·ªÉ l√†m h∆°n:
- L√†m sao ƒë·ªÉ ƒë√°nh gi√° ƒë∆∞·ª£c ƒë·ªô t·ªët c·ªßa d·ª± ƒëo√°n (th√¥ng qua ph√¢n ph·ªëi d·ª± ƒëo√°n).
- Gi·∫£ s·ª≠ c√≥ nhi·ªÅu m√¥ h√¨nh, l√†m sao ƒë·ªÉ ch·ªçn m√¥ h√¨nh t·ªët (th√¥ng qua ph√¢n ph·ªëi d·ª± ƒëo√°n).
- V√† nhi·ªÅu h∆°n th·∫ø n·ªØa (m√¨nh copy t·ª´ [Comp.ai] ü•≤).

Th·∫ø nh∆∞ng h∆∞·ªõng ti·∫øp c·∫≠n c·ªßa ta v·∫´n n·∫±m ·ªü frequentist khi c√≤n d√πng maximum likelihood, gi·ªù theo bayesian, ta gi·∫£ s·ª≠ r·∫±ng tham s·ªë $\mathbf{w}$ l√† m·ªôt bi·∫øn ng·∫´u nhi√™n v√† c√≥ ph√¢n ph·ªëi l√†:
$$
p(\mathbf{w} \mid \alpha) = \mathcal{N}(\mathbf{w} \mid \mathbf{0}, \alpha^{-1}\mathbf{I}) = \left( \frac{\alpha}{2\pi} \right)^{(M+1) / 2} \exp\left\{-\frac{\alpha}{2} \mathbf{w}^T \mathbf{w} \right\}
$$
v·ªõi $\alpha$ l√† ƒë·ªô ch√≠nh x√°c c·ªßa ph√¢n ph·ªëi v√† $M+1$ l√† s·ªë ph·∫ßn t·ª≠ c·ªßa $\mathbf{w}$ ($\mathbf{w}$ l√† h·ªá s·ªë c·ªßa ƒëa th·ª©c b·∫≠c $M$).

>[!note]+
>Nh·ªõ l·∫°i ph√¢n ph·ªëi Gaussian cho m·ªôt vector ng·∫´u nhi√™n $D$ chi·ªÅu ·ªü [[Notes/Gaussian Distribution\|Gaussian Distribution]], ta c√≥:
>$$
>\mathcal{N}(\mathbf{x} \mid \pmb{\mu}, \pmb{\Sigma}) = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\pmb{\Sigma}|^{1/2}} \exp \left\{ -\frac{1}{2} (\mathbf{x} - \pmb{\mu})^T \pmb{\Sigma}^{-1} (\mathbf{x} - \pmb{\mu}) \right\}
>$$
>V·ªõi $\pmb{\mu}$ l√† vector trung b√¨nh, ·ªü ƒë√¢y ta gi·∫£ s·ª≠ $\mathbf{w}$ c√≥ ph√¢n ph·ªëi chu·∫©n t·∫Øc, do ƒë√≥ $\pmb{\mu}=\mathbf{0}$.  C√≤n $\pmb{\Sigma}$ l√† ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai c·ªßa ph√¢n ph·ªëi. Ta ƒë·ªãnh nghƒ©a ƒë·ªô ch√≠nh x√°c c·ªßa ph√¢n ph·ªëi l√† $\pmb{\beta} = \pmb{\Sigma}^{-1}$ hay ${} \pmb{\beta}^{-1} = \pmb{\Sigma} {}$. N·∫øu ch·ªçn $\pmb{\beta} = \alpha \mathbf{I}$ v·ªõi $\mathbf{I}$ l√† ma tr·∫≠n ƒë∆°n v·ªã th√¨ ${} \pmb{\Sigma} = \pmb{\beta}^{-1} = \alpha^{-1} \mathbf{I}$ do ƒë√≥ $|\pmb{\Sigma}| = (\alpha^{-1})^{M+1} = \alpha^{-(M+1)} {}$ (b·ªüi v√¨ ${} \pmb{\Sigma}$ l√† m·ªôt ma tr·∫≠n ch√©o, v√† ƒë·ªãnh th·ª©c ma tr·∫≠n ch√©o b·∫±ng t√≠ch c√°c ph·∫ßn t·ª≠ tr√™n ƒë∆∞·ªùng ch√©o).
>
>Cu·ªëi c√πng thay $\mathbf{w}$ c√≥ chi·ªÅu $M+1$ cho $\mathbf{x}$, ta ƒë∆∞·ª£c:
>$$
\begin{align}
\mathcal{N}(\mathbf{\mathbf{w}} \mid \mathbf{0}, \alpha^{-1}\mathbf{I}) &= \frac{1}{(2\pi)^{M+1/2}} \frac{1}{(\alpha^{-(M+1)})^{1/2}} \exp\left\{ -\frac{1}{2}\mathbf{w}^T \alpha \pmb{I} \mathbf{w} \right\} \\ \\
&= \left( \frac{\alpha}{2\pi} \right)^{(M+1)/2} \exp\left\{ -\frac{\alpha}{2}\mathbf{w}^T\mathbf{w} \right\}
\end{align}
>$$

Ta th·∫•y khi thay ƒë·ªïi gi√° tr·ªã $\alpha$ th√¨ thay ƒë·ªïi lu√¥n c·∫£ ph√¢n ph·ªëi c·ªßa $\mathbf{w}$, do ƒë√≥ nh·ªØng gi√° tr·ªã nh∆∞ $\alpha$ ƒë∆∞·ª£c g·ªçi l√† **si√™u tham s·ªë** (hyperameter). Nh·ªØng si√™u tham s·ªë n√†y ƒë∆∞·ª£c ta ƒë·∫∑t tr∆∞·ªõc (ho·∫∑c c√≥ th·ªÉ t√¨m lu√¥n) tr∆∞·ªõc khi khi hu·∫•n luy·ªán m√¥ h√¨nh d·ª±a tr√™n d·ªØ li·ªáu c√≥ ƒë∆∞·ª£c.

S·ª≠ d·ª•ng ƒë·ªãnh l√Ω Bayes, ph√¢n ph·ªëi h·∫≠u nghi·ªám c·ªßa $\mathbf{w}$ t·ªâ l·ªá v·ªõi t√≠ch c·ªßa ph√¢n ph·ªëi ti√™n nghi·ªám v√† h√†m likelihood
$$
p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \alpha, \beta) \propto p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)p(\mathbf{w} \mid \alpha)
$$
khi ·∫•y ph√¢n ph·ªëi h·∫≠u nghi·ªám c·ªßa $\mathbf{w}$ ƒëi·ªÅu ki·ªán v·ªõi d·ªØ li·ªáu ($\mathbf{x}, \mathbf{y}, \beta$) (ch√≠nh l√† $p(\mathbf{w} \mid \mathcal{D})$ ·ªü ph·∫ßn [[Notes/Bayesian Probabilities\|Bayesian Probabilities]]) v√† si√™u tham s·ªë $\alpha$ s·∫Ω t·ªâ l·ªá v·ªõi t√≠ch c·ªßa h√†m likelihood ${} p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)$ (n√≥ ch√≠nh l√† $p(\mathcal{D} \mid \mathbf{w})$ ·ªü ph·∫ßn [[Notes/Bayesian Probabilities\|Bayesian Probabilities]] n·∫øu ta xem $\mathcal{D}$ g·ªìm $\mathbf{y}$ v√† $\mathbf{y}$ ƒëi·ªÅu ki·ªán $\mathbf{x}, \beta$) v√† ph√¢n ph·ªëi ti√™n nghi·ªám $p(\mathbf{w} \mid \alpha)$ (ph√¢n ph·ªëi x√°c su·∫•t c·ªßa $\mathbf{w}$ tr∆∞·ªõc khi quan s√°t d·ªØ li·ªáu).

>[!danger]+ Gi·∫£i th√≠ch c√≥ th·ªÉ sai (m√¨nh s·∫Ω c·ªë g·∫Øng gi·∫£i th√≠ch)
>Ta c√≥ (d√πng ƒë·ªãnh l√Ω Bayes):
>$$
\begin{aligned}
p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \alpha, \beta) &= \frac{p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \alpha, \beta)p(\mathbf{x}, \mathbf{w}, \alpha, \beta)}{p(\mathbf{x}, \mathbf{y}, \alpha, \beta)} \\
\implies p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \alpha, \beta) &\propto p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \alpha, \beta)p(\mathbf{x}, \mathbf{w}, \alpha, \beta)
\end{aligned}
>$$
>Ti·∫øp t·ª•c √°p d·ª•ng ƒë·ªãnh l√Ω Bayes cho $p(\mathbf{x}, \mathbf{w}, \alpha, \beta)$ ta c√≥:
>$$
\begin{aligned}
p(\mathbf{x}, \mathbf{w}, \alpha, \beta) &= p(\mathbf{w} \mid \mathbf{x}, \alpha, \beta)p(\mathbf{x}, \alpha, \beta) \\
\implies p(\mathbf{x}, \mathbf{w}, \alpha, \beta) &\propto p(\mathbf{w} \mid \mathbf{x}, \alpha, \beta)
\end{aligned}
>$$
>Ta c√≥ ƒë·ªãnh nghƒ©a sau, n·∫øu:
>$$
>P(A \mid B,C) = P(A \mid B)
>$$
>khi ƒë√≥ $A$ **ƒë·ªôc l·∫≠p ƒëi·ªÅu ki·ªán** v·ªõi $C$ ƒëi·ªÅu ki·ªán $B$, t·ª©c l√† $A \mid B$ ƒë·ªôc l·∫≠p v·ªõi $C \mid B$ (k√≠ hi·ªáu n√†y m√¨nh d√πng b·ª´a cho d·ªÖ hi·ªÉu ·∫•y ch·ª© n√≥ kh√¥ng ch·∫Øc l√† ƒë√∫ng ƒë√¢u ü•≤) hay sau khi quan s√°t $B$ th√¨ $A$ v·ªõi $C$ ƒë·ªôc l·∫≠p v·ªõi nhau.
>
>T·ª´ ƒë·ªãnh nghƒ©a tr√™n, n·∫øu m√¨nh xem $\mathbf{w} \mid \alpha$ ƒë·ªôc l·∫≠p ƒëi·ªÅu ki·ªán v·ªõi $\mathbf{x}, \beta \mid \alpha$ th√¨ $p(\mathbf{w} \mid \mathbf{x}, \alpha, \beta) = p(\mathbf{w} \mid \alpha)$. T∆∞∆°ng t·ª±, $\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta$ ƒë·ªôc l·∫≠p ƒëi·ªÅu ki·ªán v·ªõi $\alpha \mid \mathbf{x}, \mathbf{w}, \beta$, do ƒë√≥ $p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \alpha, \beta) = p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)$. V·∫≠y:
>$$
>p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \alpha, \beta) \propto p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)p(\mathbf{w} \mid \alpha)
>$$
>
>Tuy nhi√™n c√≥ 1 ƒëi·ªÅu m√¨nh kh√¥ng gi·∫£i th√≠ch, l√† t·∫°i sao l·∫°i ƒë·ªôc l·∫≠p ƒëi·ªÅu ki·ªán v·ªõi nhau, still a b√≠ ·∫©n üò≠.

ƒê·ªÉ t√¨m gi√° tr·ªã $\mathbf{w}$ t·ªëi ∆∞u ƒë∆∞·ª£c $p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \beta, \alpha)$ ta ph·∫£i t·ªëi ∆∞u c·∫£ hai h√†m l√† h√†m likelihood $p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)$ v√† ph√¢n ph·ªëi ti√™n nghi·ªám $p(\mathbf{w} \mid \alpha)$ (vi·ªác t·ªëi ∆∞u n√†y ƒë∆∞·ª£c g·ªçi l√† *MAP* hay *Maximum Posterior* ƒë∆∞·ª£c gi·ªõi thi·ªáu trong [[Notes/Bayesian Probabilities\|Bayesian Probabilities]], kh√°c v·ªõi maximum likelihood ch·ªâ t·ªëi ∆∞u m·ªói h√†m likelihood). T∆∞∆°ng t·ª± nh∆∞ maximum likelihood, ta s·∫Ω t·ªëi ƒëa h√†m log thay v√¨ h√†m ch√≠nh, ngo√†i ra m√¨nh c√≥ th·ªÉ d√πng h√†m log √¢m lu√¥n n√™n s·∫Ω t·ªëi thi·ªÉu thay cho t·ªëi ƒëa:
$$
\begin{aligned}
-\ln p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \beta, \alpha) &\propto \ln [p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)p(\mathbf{w} \mid \alpha) ] \\
&= -\ln p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta) + [-\ln p(\mathbf{w} \mid \alpha)]
\end{aligned}
$$
Nh∆∞ ƒë√£ bi·∫øt ·ªü ph√≠a tr√™n, m√¨nh s·∫Ω b·ªè ƒëi c√°c gi√° tr·ªã d∆∞ th·ª´a ·ªü ph·∫ßn b√™n tr√°i c·ªßa h√†m log √¢m ƒë·∫ßu ti√™n l√† b·ªè ƒëi $- \frac{N}{2} \ln 2\pi +\frac{N}{2} \ln \beta$. Ta ƒë∆∞·ª£c:
$$
\frac{\beta}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2
$$
·ªû ph√≠a b√™n ph·∫£i c≈©ng t∆∞∆°ng t·ª±, m√¨nh s·∫Ω b·ªè ƒëi c√°c gi√° tr·ªã d∆∞ th·ª´a (b·ªüi v√¨ ƒë·∫°o h√†m xu·ªëng c≈©ng b·∫±ng $0$ h·∫øt r·ªìi), ƒë·∫ßu ti√™n l√† b·ªè ƒëi $\frac{\alpha}{2\pi}^{(M+1) / 2}$ b·ªüi v√¨ ƒë√¢y l√† h·∫±ng s·ªë, ti·∫øp theo:
$$
\begin{aligned}
-\ln p(\mathbf{w} \mid \alpha) &\propto -\ln \exp\left\{ -\frac{\alpha}{2}\mathbf{w}^T\mathbf{w} \right\} \\
&= \frac{\alpha}{2} \mathbf{w}^T\mathbf{w}
\end{aligned}
$$
V·∫≠y k·∫øt h·ª£p c·∫£ hai l·∫°i, ta ƒë∆∞·ª£c ph∆∞∆°ng tr√¨nh m·ªõi c·∫ßn t·ªëi thi·ªÉu l√†:
$$
\frac{\beta}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 + \frac{\alpha}{2} \mathbf{w}^T\mathbf{w}
$$
N·∫øu nh√¨n kƒ© th√¨ ph∆∞∆°ng n√†y c√≥ d·∫°ng c·ªßa b√¨nh ph∆∞∆°ng nh·ªè nh·∫•t k√®m v·ªõi ph·∫ßn ch√≠nh quy ho√°, ta c√≥ $\mathbf{w}^T \mathbf{w} = ||\mathbf{w}||$, ti·∫øp t·ª•c ƒë·∫∑t $\lambda =\alpha / \beta$ v√† b·ªè ƒëi h·∫±ng s·ªë $\beta$ (ta c≈©ng kh√¥ng quan t√¢m ƒë·∫øn n√≥), ta c√≥:
$$
\begin{aligned}
\beta \left(\frac{1}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 + \frac{\lambda}{2} ||\mathbf{w}|| \right) \\
\implies 
\frac{1}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 + \frac{\lambda}{2} ||\mathbf{w}||
\end{aligned}
$$

---

Ph·∫ßn tr∆∞·ªõc: [[Notes/Gaussian Distribution\|Gaussian Distribution]]
Ph·∫ßn sau: [[Notes/Bayesian Curve Fitting\|Bayesian Curve Fitting]]

---
# References

- [Bishop] Pattern Recognition and Machine Learning - Bishop (chapter 1.2)
- [Comp.ai]  [comp.ai.neural-nets FAQ, Part 3 of 7: GeneralizationSection - What is Bayesian Learning? (faqs.org)](http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-7.html)
- [Stuck with handling of conditional probability in Bishop's "Pattern Recognition and Machine Learning" (1.66) - Mathematics Stack Exchange](https://math.stackexchange.com/questions/171226/stuck-with-handling-of-conditional-probability-in-bishops-pattern-recognition)