---
{"dg-publish":true,"permalink":"/notes/curve-fitting-revisited/","noteIcon":"ğŸ“","created":"2024-08-02T10:46:38.338+07:00","updated":"2024-08-02T10:47:44.750+07:00"}
---


á» pháº§n [[Notes/Polynomial Curve Fitting\|Polynomial Curve Fitting]], ta Ä‘Ã£ biáº¿t cÃ¡ch Ä‘á»ƒ xáº¥p xá»‰ má»™t hÃ m Ä‘a thá»©c $f(x, \mathbf{w}) = \sum_{n=0}^M x^n w_{n}$ báº±ng cÃ¡ch tá»‘i thiá»ƒu hÃ m máº¥t mÃ¡t
$$
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N [f(x_{n}, \mathbf{w}) - y_{n}]^2
$$
á» pháº§n nÃ y, ta cÅ©ng sáº½ lÃ m nhÆ° váº­y nhÆ°ng vá»›i gÃ³c nhÃ¬n xÃ¡c suáº¥t Bayes. Váº«n dÃ¹ng láº¡i vÃ­ dá»¥ cÅ©, giáº£ sá»­ LN thu tháº­p Ä‘Æ°á»£c má»™t bá»™ dá»¯ liá»‡u $\mathbf{x} = \{x_{1}, x_{2}, \dots, x_{N}\}^T$ sau $N$ láº§n mua, vá»›i má»—i sá»‘ tiá»n $x_{i}$, LN mua Ä‘Æ°á»£c sá»‘ bÃ¡nh xÃ¨o $y_{i}$, váº­y $\mathbf{y} = \{ y_{1}, y_{2}, \dots, y_{N}\}^T$ vÃ  LN giáº£ sá»­ dá»¯ liá»‡u tuÃ¢n theo hÃ m Ä‘a thá»©c:
$$
f(x, \mathbf{w}) = \sum_{n=0}^M x^{n} w_{n}
$$
NhÆ°ng sáº½ khÃ¡c vá»›i vÃ­ dá»¥ ban Ä‘áº§u á»Ÿ chá»— nÃ y, ta giáº£ sá»­ vá»›i má»—i sá»‘ bÃ¡nh xÃ¨o $x$, sá»‘ tiá»n $y$ tuÃ¢n theo má»™t phÃ¢n phá»‘i chuáº©n vá»›i trung bÃ¬nh lÃ  giÃ¡ trá»‹ cá»§a Ä‘a thá»©c $f(x, \mathbf{w})$ vÃ  phÆ°Æ¡ng sai lÃ  $\beta^{-1}$ (nhÆ° á»Ÿ pháº§n [[Notes/Gaussian Distribution\|Gaussian Distribution]], $\beta$ lÃ  Ä‘á»™ chÃ­nh xÃ¡c cá»§a phÃ¢n phá»‘i vÃ  $\beta = 1 / \sigma^2$). Váº­y ta cÃ³ phÃ¢n phá»‘i cá»§a $y$ vá»›i tham sá»‘ lÃ  $x, \mathbf{w}, \beta$ lÃ :
$$
p(y \mid x, \mathbf{w}, \beta) = \mathcal{N}(y \mid \mu = f(x, \mathbf{w}), \sigma^2 = \beta^{-1})
$$
>[!note]+
>MÃ¬nh khÃ´ng biáº¿t táº¡i sao tÃ¡c giáº£ láº¡i chá»n trung bÃ¬nh vÃ  phÆ°Æ¡ng sai cÃ³ giÃ¡ trá»‹ nhÆ° nÃ y ğŸ¥². Trung bÃ¬nh thÃ¬ mÃ¬nh chá»‹u, nhÆ°ng viá»‡c chá»n Ä‘á»™ chÃ­nh xÃ¡c ${} \beta^{-1} {}$ thay cho phÆ°Æ¡ng sai $\sigma^2$ sáº½ cÃ³ lá»£i Ã­ch nhÆ° nÃ y.
>$$
\begin{align}
>\mathcal{N}(y \mid \mu, \beta^{-1}) &= \frac{1}{\sqrt{ 2\pi \beta^{-1} }} \exp\left( \frac{1}{2\beta^{-1}} (y - \mu)^2 \right) \\
&= \frac{\sqrt{ \beta }}{\sqrt{ 2\pi }} \exp\left( \frac{\beta}{2}(y -\mu)^2 \right)
\end{align}
>$$
>CÃ³ thá»ƒ tháº¥y cÃ¡c tham sá»‘ Ä‘á»u cá»§a phÃ¢n phá»‘i chuáº©n Ä‘á»u náº±m á»Ÿ trÃªn tá»­, Ä‘iá»u nÃ y sáº½ giÃºp cho viá»‡c tÃ­nh toÃ¡n dá»… dÃ ng hÆ¡n.

Giá» LN sáº½ sá»­ dá»¥ng bá»™ dá»¯ liá»‡u Ä‘Ã£ cÃ³ $\{\mathbf{x}, \mathbf{y}\}$ Ä‘á»ƒ tÃ¬m giÃ¡ trá»‹ $\mathbf{w}$ vÃ  $\beta$ sao cho $p(y \mid \mathbf{w}, \beta)$ lÃ  lá»›n nháº¥t báº±ng cÃ¡ch dÃ¹ng maximum log likehood. Giáº£ sá»­ ráº±ng bá»™ dá»¯ liá»‡u cá»§a LN Ä‘Æ°á»£c láº¥y ra má»™t cÃ¡ch Ä‘á»™c láº­p tá»« phÃ¢n phá»‘i chuáº©n $\mathcal{N}(y \mid f(x, \mathbf{w}), \beta^{-1})$, ta cÃ³:
$$
\begin{align}
\ln p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta) &= \ln \prod_{n=1}^N p(y_{n} \mid x_{n}, \mathbf{w}, \beta) \\ 
&= \sum_{n=1}^N \ln p(y_{n} \mid x_{n}, \mathbf{w}, \beta) \\
&= \sum_{n=1}^N \ln \mathcal{N}(y_{n} \mid f(x_{n}, \mathbf{w}), \beta^{-1})
\end{align}
$$
á» [[Notes/Maximum Log likelihood\|Maximum Log likelihood]], ta Ä‘Ã£ Ä‘Æ°a ra cÃ´ng thá»©c tá»•ng quÃ¡t cho $\sum_{x} \ln \mathcal{N}(x \mid \mu, \sigma^2)$, giá» chá»‰ cáº§n thay $x = y_{n}, \mu = f(x_{n}, \mathbf{w})$ vÃ  $\sigma^2 = \beta^{-1}$, ta Ä‘Æ°á»£c:
$$
\begin{align}
\sum_{n=1}^N \ln \mathcal{N}(y_{n} \mid f(x_{n}, \mathbf{w}), \beta^{-1}) &= \left[ -\frac{1}{2\beta^{-1}} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 \right] - \frac{N}{2}\ln 2\pi - \frac{N}{2} \ln \beta^{-1} \\
&= \left[ -\frac{\beta}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 \right] - \frac{N}{2} \ln 2\pi +\frac{N}{2} \ln \beta
\end{align}
$$
Äáº§u tiÃªn Ä‘á»‘i vá»›i tham sá»‘ $\mathbf{w}$, ta tháº¥y $-\frac{N}{2} \ln 2\pi$ vÃ  $\frac{N}{2} \ln \beta$ lÃ  cÃ¡c giÃ¡ trá»‹ khÃ´ng liÃªn quan Ä‘áº¿n $\mathbf{w}$, khi Ä‘áº¡o hÃ m báº±ng $0$ do Ä‘Ã³ ta cÃ³ thá»ƒ bá» Ä‘i hai giÃ¡ trá»‹ Ä‘Ã³, ngoÃ i ra giÃ¡ trá»‹ $-\frac{\beta}{2}$ lÃ  má»™t háº±ng sá»‘, khi Ä‘áº¡o hÃ m vÃ  láº¥y báº±ng $0$ ta khÃ´ng cáº§n xÃ©t Ä‘áº¿n háº±ng sá»‘ Ä‘Ã³, ta cÃ³ thá»ƒ loáº¡i bá» luÃ´n Ä‘i $\beta$ vÃ  chá»‰ giá»¯ láº¡i $-\frac{1}{2}$ (má»¥c Ä‘Ã­ch Ä‘á»ƒ cho giá»‘ng bÃ i toÃ¡n least square). Váº­y má»¥c Ä‘Ã­ch cá»§a ta lÃ  tÃ¬m $\mathbf{w}$ sao cho hÃ m dÆ°á»›i Ä‘Ã¢y lÃ  lá»›n nháº¥t:
$$
-\frac{1}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2
$$
Äá»ƒ cá»±c Ä‘áº¡i hÃ m log thÃ¬ ta cÃ³ thá»ƒ cá»±c tiá»ƒu hÃ m log Ã¢m, váº­y ta tÃ¬m $\mathbf{w}$ sao cho hÃ m dÆ°á»›i Ä‘Ã¢y lÃ  nhá» nháº¥t:
$$
\frac{1}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2
$$
CÃ³ thá»ƒ tháº¥y, bÃ i toÃ¡n tÃ¬m $\mathbf{w}$ sao cho log likehoood cá»±c Ä‘áº¡i Ä‘Ã£ Ä‘Æ°a vá» thÃ nh bÃ i toÃ¡n bÃ¬nh phÆ°Æ¡ng nhá» nháº¥t (least square) nhÆ° á»Ÿ bÃ i [[Notes/Polynomial Curve Fitting\|Polynomial Curve Fitting]]. Äáº·t giÃ¡ trá»‹ tá»‘i Æ°u $\mathbf{w}$ mÃ  ta tÃ¬m Ä‘Æ°á»£c lÃ  $\mathbf{w}_{ML}$.

CÃ²n Ä‘á»‘i vá»›i tham sá»‘ $\beta$, náº¿u láº¥y Ä‘áº¡o hÃ m cá»§a log likelihood, ta Ä‘Æ°á»£c:
$$
-\frac{1}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 + \frac{N}{2} \frac{1}{\beta}
$$
Váº­y Ä‘á»ƒ phÆ°Æ¡ng trÃ¬nh trÃªn báº±ng $0$, ta cÃ³ giÃ¡ trá»‹ tá»‘i Æ°u $\beta_{ML}$ lÃ :
$$
\frac{1}{\beta_{ML}} = \frac{1}{N} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}_{ML}))^2
$$
Do Ä‘Ã£ cÃ³ hai tham sá»‘ $\mathbf{w}$ vÃ  $\beta$ cáº§n thiáº¿t lÃ  $\mathbf{w}_{ML}$ vÃ  $\beta_{ML}$ thÃ´ng qua maximum likelihood, ta cÃ³ Ä‘Æ°á»£c phÃ¢n phá»‘i:
$$
p(y \mid x, \mathbf{w}_{ML}, \beta_{ML}) = \mathcal{N}(y \mid f(x, \mathbf{w}_{ML}), \beta^{-1}_{ML})
$$
ta gá»i phÃ¢n phá»‘i trÃªn lÃ  **phÃ¢n phá»‘i dá»± Ä‘oÃ¡n** (predictive distribution). KhÃ¡c vá»›i pháº§n [[Notes/Polynomial Curve Fitting\|Polynomial Curve Fitting]], ta khÃ´ng tÃ¬m má»™t giÃ¡ trá»‹ $y$ cá»¥ thá»ƒ vá»›i $x$, mÃ  ta tÃ¬m Ä‘Æ°á»£c cáº£ má»™t phÃ¢n phá»‘i cá»§a $y$, khi thay má»™t giÃ¡ trá»‹ $x_{0}$ báº¥t kÃ¬ vÃ o, ta tÃ¬m Ä‘Æ°á»£c phÃ¢n phá»‘i $p(y \mid x_{0}, \mathbf{w}_{ML}, \beta_{ML})$. 

Váº­y lÃ m sao Ä‘á»ƒ tÃ¬m giÃ¡ trá»‹ $y_{0}$ náº¿u thay tháº¿ giÃ¡ trá»‹ $x_{0}$ cho cáº£ má»™t phÃ¢n phá»‘i thay vÃ¬ má»™t giÃ¡ trá»‹, thÃ´ng thÆ°á»ng ngÆ°á»i ta sáº½ láº¥y trung bÃ¬nh cá»§a phÃ¢n phá»‘i $p(y \mid x_{0}, \mathbf{w}_{ML}, \beta_{ML})$, tá»©c lÃ  $y_{0} = f(x_{0}, \mathbf{w}_{ML})$ (Ã  thÃ¬ ra Ä‘áº¥y lÃ  lÃ½ do táº¡i sao chá»n trung bÃ¬nh lÃ  ${} f(x, \mathbf{w})$ ğŸ’€). Váº­y cÅ©ng quay ngÆ°á»£c láº¡i giá»‘ng vá»›i pháº§n [[Notes/Polynomial Curve Fitting\|Polynomial Curve Fitting]], tháº¿ nhÆ°ng viá»‡c cÃ³ thÃªm phÃ¢n phá»‘i dá»± Ä‘oÃ¡n, ta cÃ³ Ä‘Æ°á»£c nhiá»u thá»© Ä‘á»ƒ lÃ m hÆ¡n:
- LÃ m sao Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c Ä‘á»™ tá»‘t cá»§a dá»± Ä‘oÃ¡n (thÃ´ng qua phÃ¢n phá»‘i dá»± Ä‘oÃ¡n).
- Giáº£ sá»­ cÃ³ nhiá»u mÃ´ hÃ¬nh, lÃ m sao Ä‘á»ƒ chá»n mÃ´ hÃ¬nh tá»‘t (thÃ´ng qua phÃ¢n phá»‘i dá»± Ä‘oÃ¡n).
- VÃ  nhiá»u hÆ¡n tháº¿ ná»¯a (mÃ¬nh copy tá»« [Comp.ai] ğŸ¥²).

Tháº¿ nhÆ°ng hÆ°á»›ng tiáº¿p cáº­n cá»§a ta váº«n náº±m á»Ÿ frequentist khi cÃ²n dÃ¹ng maximum likelihood, giá» theo bayesian, ta giáº£ sá»­ ráº±ng tham sá»‘ $\mathbf{w}$ lÃ  má»™t biáº¿n ngáº«u nhiÃªn vÃ  cÃ³ phÃ¢n phá»‘i lÃ :
$$
p(\mathbf{w} \mid \alpha) = \mathcal{N}(\mathbf{w} \mid \mathbf{0}, \alpha^{-1}\mathbf{I}) = \left( \frac{\alpha}{2\pi} \right)^{(M+1) / 2} \exp\left\{-\frac{\alpha}{2} \mathbf{w}^T \mathbf{w} \right\}
$$
vá»›i $\alpha$ lÃ  Ä‘á»™ chÃ­nh xÃ¡c cá»§a phÃ¢n phá»‘i vÃ  $M+1$ lÃ  sá»‘ pháº§n tá»­ cá»§a $\mathbf{w}$ ($\mathbf{w}$ lÃ  há»‡ sá»‘ cá»§a Ä‘a thá»©c báº­c $M$).

>[!note]+
>Nhá»› láº¡i phÃ¢n phá»‘i Gaussian cho má»™t vector ngáº«u nhiÃªn $D$ chiá»u á»Ÿ [[Notes/Gaussian Distribution\|Gaussian Distribution]], ta cÃ³:
>$$
>\mathcal{N}(\mathbf{x} \mid \pmb{\mu}, \pmb{\Sigma}) = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\pmb{\Sigma}|^{1/2}} \exp \left\{ -\frac{1}{2} (\mathbf{x} - \pmb{\mu})^T \pmb{\Sigma}^{-1} (\mathbf{x} - \pmb{\mu}) \right\}
>$$
>Vá»›i $\pmb{\mu}$ lÃ  vector trung bÃ¬nh, á»Ÿ Ä‘Ã¢y ta giáº£ sá»­ $\mathbf{w}$ cÃ³ phÃ¢n phá»‘i chuáº©n táº¯c, do Ä‘Ã³ $\pmb{\mu}=\mathbf{0}$.  CÃ²n $\pmb{\Sigma}$ lÃ  ma tráº­n hiá»‡p phÆ°Æ¡ng sai cá»§a phÃ¢n phá»‘i. Ta Ä‘á»‹nh nghÄ©a Ä‘á»™ chÃ­nh xÃ¡c cá»§a phÃ¢n phá»‘i lÃ  $\pmb{\beta} = \pmb{\Sigma}^{-1}$ hay ${} \pmb{\beta}^{-1} = \pmb{\Sigma} {}$. Náº¿u chá»n $\pmb{\beta} = \alpha \mathbf{I}$ vá»›i $\mathbf{I}$ lÃ  ma tráº­n Ä‘Æ¡n vá»‹ thÃ¬ ${} \pmb{\Sigma} = \pmb{\beta}^{-1} = \alpha^{-1} \mathbf{I}$ do Ä‘Ã³ $|\pmb{\Sigma}| = (\alpha^{-1})^{M+1} = \alpha^{-(M+1)} {}$ (bá»Ÿi vÃ¬ ${} \pmb{\Sigma}$ lÃ  má»™t ma tráº­n chÃ©o, vÃ  Ä‘á»‹nh thá»©c ma tráº­n chÃ©o báº±ng tÃ­ch cÃ¡c pháº§n tá»­ trÃªn Ä‘Æ°á»ng chÃ©o).
>
>Cuá»‘i cÃ¹ng thay $\mathbf{w}$ cÃ³ chiá»u $M+1$ cho $\mathbf{x}$, ta Ä‘Æ°á»£c:
>$$
\begin{align}
\mathcal{N}(\mathbf{\mathbf{w}} \mid \mathbf{0}, \alpha^{-1}\mathbf{I}) &= \frac{1}{(2\pi)^{M+1/2}} \frac{1}{(\alpha^{-(M+1)})^{1/2}} \exp\left\{ -\frac{1}{2}\mathbf{w}^T \alpha \pmb{I} \mathbf{w} \right\} \\ \\
&= \left( \frac{\alpha}{2\pi} \right)^{(M+1)/2} \exp\left\{ -\frac{\alpha}{2}\mathbf{w}^T\mathbf{w} \right\}
\end{align}
>$$

Ta tháº¥y khi thay Ä‘á»•i giÃ¡ trá»‹ $\alpha$ thÃ¬ thay Ä‘á»•i luÃ´n cáº£ phÃ¢n phá»‘i cá»§a $\mathbf{w}$, do Ä‘Ã³ nhá»¯ng giÃ¡ trá»‹ nhÆ° $\alpha$ Ä‘Æ°á»£c gá»i lÃ  **siÃªu tham sá»‘** (hyperameter). Nhá»¯ng siÃªu tham sá»‘ nÃ y Ä‘Æ°á»£c ta Ä‘áº·t trÆ°á»›c (hoáº·c cÃ³ thá»ƒ tÃ¬m luÃ´n) trÆ°á»›c khi khi huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»±a trÃªn dá»¯ liá»‡u cÃ³ Ä‘Æ°á»£c.

Sá»­ dá»¥ng Ä‘á»‹nh lÃ½ Bayes, phÃ¢n phá»‘i háº­u nghiá»‡m cá»§a $\mathbf{w}$ tá»‰ lá»‡ vá»›i tÃ­ch cá»§a phÃ¢n phá»‘i tiÃªn nghiá»‡m vÃ  hÃ m likelihood
$$
p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \alpha, \beta) \propto p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)p(\mathbf{w} \mid \alpha)
$$
khi áº¥y phÃ¢n phá»‘i háº­u nghiá»‡m cá»§a $\mathbf{w}$ Ä‘iá»u kiá»‡n vá»›i dá»¯ liá»‡u ($\mathbf{x}, \mathbf{y}, \beta$) (chÃ­nh lÃ  $p(\mathbf{w} \mid \mathcal{D})$ á»Ÿ pháº§n [[Notes/Bayesian Probabilities\|Bayesian Probabilities]]) vÃ  siÃªu tham sá»‘ $\alpha$ sáº½ tá»‰ lá»‡ vá»›i tÃ­ch cá»§a hÃ m likelihood ${} p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)$ (nÃ³ chÃ­nh lÃ  $p(\mathcal{D} \mid \mathbf{w})$ á»Ÿ pháº§n [[Notes/Bayesian Probabilities\|Bayesian Probabilities]] náº¿u ta xem $\mathcal{D}$ gá»“m $\mathbf{y}$ vÃ  $\mathbf{y}$ Ä‘iá»u kiá»‡n $\mathbf{x}, \beta$) vÃ  phÃ¢n phá»‘i tiÃªn nghiá»‡m $p(\mathbf{w} \mid \alpha)$ (phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a $\mathbf{w}$ trÆ°á»›c khi quan sÃ¡t dá»¯ liá»‡u).

>[!danger]+ Giáº£i thÃ­ch cÃ³ thá»ƒ sai (mÃ¬nh sáº½ cá»‘ gáº¯ng giáº£i thÃ­ch)
>Ta cÃ³ (dÃ¹ng Ä‘á»‹nh lÃ½ Bayes):
>$$
\begin{aligned}
p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \alpha, \beta) &= \frac{p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \alpha, \beta)p(\mathbf{x}, \mathbf{w}, \alpha, \beta)}{p(\mathbf{x}, \mathbf{y}, \alpha, \beta)} \\
\implies p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \alpha, \beta) &\propto p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \alpha, \beta)p(\mathbf{x}, \mathbf{w}, \alpha, \beta)
\end{aligned}
>$$
>Tiáº¿p tá»¥c Ã¡p dá»¥ng Ä‘á»‹nh lÃ½ Bayes cho $p(\mathbf{x}, \mathbf{w}, \alpha, \beta)$ ta cÃ³:
>$$
\begin{aligned}
p(\mathbf{x}, \mathbf{w}, \alpha, \beta) &= p(\mathbf{w} \mid \mathbf{x}, \alpha, \beta)p(\mathbf{x}, \alpha, \beta) \\
\implies p(\mathbf{x}, \mathbf{w}, \alpha, \beta) &\propto p(\mathbf{w} \mid \mathbf{x}, \alpha, \beta)
\end{aligned}
>$$
>Ta cÃ³ Ä‘á»‹nh nghÄ©a sau, náº¿u:
>$$
>P(A \mid B,C) = P(A \mid B)
>$$
>khi Ä‘Ã³ $A$ **Ä‘á»™c láº­p Ä‘iá»u kiá»‡n** vá»›i $C$ Ä‘iá»u kiá»‡n $B$, tá»©c lÃ  $A \mid B$ Ä‘á»™c láº­p vá»›i $C \mid B$ (kÃ­ hiá»‡u nÃ y mÃ¬nh dÃ¹ng bá»«a cho dá»… hiá»ƒu áº¥y chá»© nÃ³ khÃ´ng cháº¯c lÃ  Ä‘Ãºng Ä‘Ã¢u ğŸ¥²) hay sau khi quan sÃ¡t $B$ thÃ¬ $A$ vá»›i $C$ Ä‘á»™c láº­p vá»›i nhau.
>
>Tá»« Ä‘á»‹nh nghÄ©a trÃªn, náº¿u mÃ¬nh xem $\mathbf{w} \mid \alpha$ Ä‘á»™c láº­p Ä‘iá»u kiá»‡n vá»›i $\mathbf{x}, \beta \mid \alpha$ thÃ¬ $p(\mathbf{w} \mid \mathbf{x}, \alpha, \beta) = p(\mathbf{w} \mid \alpha)$. TÆ°Æ¡ng tá»±, $\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta$ Ä‘á»™c láº­p Ä‘iá»u kiá»‡n vá»›i $\alpha \mid \mathbf{x}, \mathbf{w}, \beta$, do Ä‘Ã³ $p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \alpha, \beta) = p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)$. Váº­y:
>$$
>p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \alpha, \beta) \propto p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)p(\mathbf{w} \mid \alpha)
>$$
>
>Tuy nhiÃªn cÃ³ 1 Ä‘iá»u mÃ¬nh khÃ´ng giáº£i thÃ­ch, lÃ  táº¡i sao láº¡i Ä‘á»™c láº­p Ä‘iá»u kiá»‡n vá»›i nhau, still a bÃ­ áº©n ğŸ˜­.

Äá»ƒ tÃ¬m giÃ¡ trá»‹ $\mathbf{w}$ tá»‘i Æ°u Ä‘Æ°á»£c $p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \beta, \alpha)$ ta pháº£i tá»‘i Æ°u cáº£ hai hÃ m lÃ  hÃ m likelihood $p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)$ vÃ  phÃ¢n phá»‘i tiÃªn nghiá»‡m $p(\mathbf{w} \mid \alpha)$ (viá»‡c tá»‘i Æ°u nÃ y Ä‘Æ°á»£c gá»i lÃ  *MAP* hay *Maximum Posterior* Ä‘Æ°á»£c giá»›i thiá»‡u trong [[Notes/Bayesian Probabilities\|Bayesian Probabilities]], khÃ¡c vá»›i maximum likelihood chá»‰ tá»‘i Æ°u má»—i hÃ m likelihood). TÆ°Æ¡ng tá»± nhÆ° maximum likelihood, ta sáº½ tá»‘i Ä‘a hÃ m log thay vÃ¬ hÃ m chÃ­nh, ngoÃ i ra mÃ¬nh cÃ³ thá»ƒ dÃ¹ng hÃ m log Ã¢m luÃ´n nÃªn sáº½ tá»‘i thiá»ƒu thay cho tá»‘i Ä‘a:
$$
\begin{aligned}
-\ln p(\mathbf{w} \mid \mathbf{x}, \mathbf{y}, \beta, \alpha) &\propto \ln [p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta)p(\mathbf{w} \mid \alpha) ] \\
&= -\ln p(\mathbf{y} \mid \mathbf{x}, \mathbf{w}, \beta) + [-\ln p(\mathbf{w} \mid \alpha)]
\end{aligned}
$$
NhÆ° Ä‘Ã£ biáº¿t á»Ÿ phÃ­a trÃªn, mÃ¬nh sáº½ bá» Ä‘i cÃ¡c giÃ¡ trá»‹ dÆ° thá»«a á»Ÿ pháº§n bÃªn trÃ¡i cá»§a hÃ m log Ã¢m Ä‘áº§u tiÃªn lÃ  bá» Ä‘i $- \frac{N}{2} \ln 2\pi +\frac{N}{2} \ln \beta$. Ta Ä‘Æ°á»£c:
$$
\frac{\beta}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2
$$
á» phÃ­a bÃªn pháº£i cÅ©ng tÆ°Æ¡ng tá»±, mÃ¬nh sáº½ bá» Ä‘i cÃ¡c giÃ¡ trá»‹ dÆ° thá»«a (bá»Ÿi vÃ¬ Ä‘áº¡o hÃ m xuá»‘ng cÅ©ng báº±ng $0$ háº¿t rá»“i), Ä‘áº§u tiÃªn lÃ  bá» Ä‘i $\frac{\alpha}{2\pi}^{(M+1) / 2}$ bá»Ÿi vÃ¬ Ä‘Ã¢y lÃ  háº±ng sá»‘, tiáº¿p theo:
$$
\begin{aligned}
-\ln p(\mathbf{w} \mid \alpha) &\propto -\ln \exp\left\{ -\frac{\alpha}{2}\mathbf{w}^T\mathbf{w} \right\} \\
&= \frac{\alpha}{2} \mathbf{w}^T\mathbf{w}
\end{aligned}
$$
Váº­y káº¿t há»£p cáº£ hai láº¡i, ta Ä‘Æ°á»£c phÆ°Æ¡ng trÃ¬nh má»›i cáº§n tá»‘i thiá»ƒu lÃ :
$$
\frac{\beta}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 + \frac{\alpha}{2} \mathbf{w}^T\mathbf{w}
$$
Náº¿u nhÃ¬n kÄ© thÃ¬ phÆ°Æ¡ng nÃ y cÃ³ dáº¡ng cá»§a bÃ¬nh phÆ°Æ¡ng nhá» nháº¥t kÃ¨m vá»›i pháº§n chÃ­nh quy hoÃ¡, ta cÃ³ $\mathbf{w}^T \mathbf{w} = ||\mathbf{w}||$, tiáº¿p tá»¥c Ä‘áº·t $\lambda =\alpha / \beta$ vÃ  bá» Ä‘i háº±ng sá»‘ $\beta$ (ta cÅ©ng khÃ´ng quan tÃ¢m Ä‘áº¿n nÃ³), ta cÃ³:
$$
\begin{aligned}
\beta \left(\frac{1}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 + \frac{\lambda}{2} ||\mathbf{w}|| \right) \\
\implies 
\frac{1}{2} \sum_{n=1}^N (y_{n} - f(x_{n}, \mathbf{w}))^2 + \frac{\lambda}{2} ||\mathbf{w}||
\end{aligned}
$$

---

Pháº§n trÆ°á»›c: [[Notes/Gaussian Distribution\|Gaussian Distribution]]
Pháº§n sau: [[Notes/Bayesian Curve Fitting\|Bayesian Curve Fitting]]

---
# References

- [Bishop] Pattern Recognition and Machine Learning - Bishop (chapter 1.2)
- [Comp.ai]  [comp.ai.neural-nets FAQ, Part 3 of 7: GeneralizationSection - What is Bayesian Learning? (faqs.org)](http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-7.html)
- [Stuck with handling of conditional probability in Bishop's "Pattern Recognition and Machine Learning" (1.66) - Mathematics Stack Exchange](https://math.stackexchange.com/questions/171226/stuck-with-handling-of-conditional-probability-in-bishops-pattern-recognition)