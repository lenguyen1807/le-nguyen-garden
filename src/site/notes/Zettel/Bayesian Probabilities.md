---
{"dg-publish":true,"permalink":"/zettel/bayesian-probabilities/","noteIcon":"üìù","created":"2024-06-30T19:11:48.440+07:00","updated":"2024-07-12T08:26:56.139+07:00"}
---


ƒê·ªãnh l√Ω Bayes c≈©ng c√≥ th·ªÉ √°p d·ª•ng trong Machine Leanring nh∆∞ sau. Gi·∫£ s·ª≠ r·∫±ng ta c√≥ t·∫≠p d·ªØ li·ªáu $\mathcal{D}$ v√† m·ªôt model c√≥ b·ªô tham s·ªë l√† $\mathbf{w}$. M·ª•c ƒë√≠ch c·ªßa ta l√† v·ªõi t·∫≠p d·ªØ li·ªáu $\mathcal{D}$ nh∆∞ n√†y, b·ªô tham s·ªë $\mathbf{w}$ c·ªßa ta nh∆∞ n√†o m·ªõi l√† t·ªët, t·ª©c l√† t√¨m x√°c su·∫•t $p(\mathbf{w} \mid \mathcal{D})$.

Gi·∫£ s·ª≠ ta ƒë√£ ch·ªçn ƒë∆∞·ª£c m·ªôt m√¥ h√¨nh v·ªõi b·ªô tham s·ªë $\mathbf{w}$. Tr∆∞·ªõc khi c√≥ t·∫≠p d·ªØ li·ªáu $\mathcal{D}$, ta ng·∫ßm gi·∫£ ƒë·ªãnh r·∫±ng $\mathbf{w}$ c√≥ ph√¢n ph·ªëi l√† $p(\mathbf{w})$. S·ª≠ d·ª•ng ƒë·ªãnh l√Ω Bayes, ta c√≥:
$$
p(\mathbf{w} \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \mathbf{w})p(\mathbf{w})}{p(\mathcal{D})}
$$
Ph√¢n ph·ªëi $p(\mathcal{D} \mid \mathbf{w})$ ·ªü b√™n ph·∫£i c·ªßa ƒë·ªãnh l√Ω Bayes l√† m·ªôt h√†m ph·ª• thu·ªôc v√†o $\mathcal{D}$, th·∫ø nh∆∞ng n·∫øu ta xem ph√¢n ph·ªëi ·∫•y l√† m·ªôt h√†m ph·ª• thu·ªôc v√†o $\mathbf{w}$, th√¨ khi ƒë√≥ ta g·ªçi $p(\mathcal{D} \mid \mathbf{w})$ l√† **h√†m likelihood** (likelihood function).

>[!note]+
>Thay v√¨ vi·∫øt $p(\mathcal{D} \mid \mathbf{w})$ ƒë·ªÉ d·ªÖ b·ªã nh·∫ßm l·∫´n gi·ªØa ph√¢n ph·ªëi x√°c su·∫•t v√† h√†m likelihood, ta s·∫Ω k√≠ hi·ªáu:
>$$
\mathcal{L}(\mathbf{w} \mid \mathcal{D}) = p(\mathcal{D} \mid \mathbf{w})
>$$
>trong ƒë√≥ $\mathcal{L}(\mathbf{w} \mid \mathcal{D})$ l√† h√†m likelihood c√≥ bi·∫øn l√† $\mathbf{w}$ c√≤n $p(\mathcal{D} \mid \mathbf{w})$ l√† ph√¢n ph·ªëi c√≥ bi·∫øn l√† $\mathcal{D}$.

D·ª±a v√†o ƒë·ªãnh nghƒ©a c·ªßa likelihood, ta c√≥ th·ªÉ vi·∫øt l·∫°i ƒë·ªãnh l√Ω Bayes nh∆∞ sau:
$$
\text{posterior} \propto \text{likelihood} \times \text{prior}
$$
>[!note]+
>K√≠ hi·ªáu $\propto$ nghƒ©a l√† t·ªâ l·ªá. V√≠ d·ª•, chi·ªÅu cao ($h$) $= 1.2 \times$ c√¢n n·∫∑ng ($w$), l√∫c n√†y ta n√≥i chi·ªÅu cao t·ªâ l·ªá v·ªõi c√¢n n·∫∑ng hay $h \propto w$. ·ªû ƒë·ªãnh l√Ω Bayes, n·∫øu ta xem $1 / p(\mathcal{D})$ l√† m·ªôt h·∫±ng s·ªë (m√† n√≥ l√† m·ªôt h·∫±ng s·ªë th·∫≠t, b·ªüi v√¨ $\mathcal{D}$ kh√¥ng ƒë·ªïi r·ªìi) th√¨ $p(\mathbf{w} \mid \mathcal{D}) \propto \mathcal{L}(\mathbf{w} \mid \mathcal{D})p(\mathbf{w})$.

trong ƒë√≥ $\text{likelihood}, \text{posterior}$ v√† $\text{prior}$ ƒë·ªÅu l√† c√°c h√†m ph·ª• thu·ªôc v√†o $\mathbf{w}$. C√≤n gi√° tr·ªã $p(\mathcal{D})$ d∆∞·ªõi m·∫´u l√† m·ªôt h·∫±ng s·ªë, d√πng ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng ph√¢n ph·ªëi h·∫≠u nghi·ªám ·ªü b√™n ph·∫£i ƒë·ªãnh l√Ω Bayes ƒë√∫ng l√† m·ªôt ph√¢n ph·ªëi (m·∫≠t ƒë·ªô x√°c su·∫•t).

>[!note]+
>√Åp d·ª•ng sum rule v√† product rule ([[Zettel/Probability Densities\|Probability Densities]]) cho bi·∫øn t·ª•c, ta c√≥:
>$$
>p(\mathcal{D}) = \int p(\mathcal{D} \mid \mathbf{w}) p(\mathbf{w}) d\mathbf{w}
>$$
>V·∫≠y:
>$$
\begin{aligned}
\int_{-\infty}^{\infty} p(\mathbf{w} \mid \mathcal{D}) d\mathbf{w} &= \int_{-\infty}^{\infty} \frac{p(\mathcal{D} \mid \mathbf{w})p(\mathbf{w})}{p(\mathcal{D})} d\mathbf{w} \\
&= \frac{\int p(\mathcal{D}\mid \mathbf{w})p(\mathbf{w}) d\mathbf{w}}{\int p(\mathcal{D}\mid \mathbf{w})p(\mathbf{w}) d\mathbf{w}} = 1
\end{aligned}
>$$
>C√≤n vi·ªác $p(\mathbf{w} \mid \mathcal{D}) \geq 0$ m√¨nh nghƒ© kh√° l√† d·ªÖ th·∫•y r·ªìi.

V·∫≠y sinh ra c√°i Bayes n√†y l√†m g√¨, m·ª•c ƒë√≠ch c·ªßa ch√∫ng ta ƒë√≥ l√† c·ªë g·∫Øng t√¨m b·ªô tham s·ªë $\mathbf{w}$ sao cho $p(\mathbf{w} \mid \mathcal{D})$ l√† t·ªët nh·∫•t, ki·ªÉu nh∆∞ ta ƒë√£ bi·∫øt tr∆∞·ªõc t·∫≠p d·ªØ li·ªáu $\mathcal{D}$ (t·ª©c l√† ta ƒë√£ bi·∫øt tr∆∞·ªõc k·∫øt qu·∫£ r·ªìi), ta c·∫ßn t√¨m m√¥ h√¨nh (tham s·ªë $\mathbf{w}$) ph√π h·ª£p v·ªõi $\mathcal{D}$ nh·∫•t (t·ª©c l√† ta ƒëi t√¨m nguy√™n nh√¢n cho ra k·∫øt qu·∫£ v√† nguy√™n nh√¢n ƒë√≥ ph·∫£i l√† ph√π h·ª£p v·ªõi k·∫øt qu·∫£ nh·∫•t, v·∫≠y l√† t√¨m x√°c su·∫•t $p(\mathbf{w}\mid \mathcal{D})$ l·ªõn nh·∫•t) (m√¨nh copy c√°ch gi·∫£i th√≠ch n√†y t·ª´ [VHT]).

>[!note]+
>Ngo√†i c√°ch gi·∫£i th√≠ch tr√™n, ta h√£y xem x√°c su·∫•t nh∆∞ *m·ª©c ƒë·ªô c·ªßa ni·ªÅm tin* (degree of belief) t·ª©c l√† x√°c su·∫•t c√†ng cao, ta c√†ng tin l√† n√≥ s·∫Ω t·ªët (ho·∫∑c s·∫Ω x·∫£y ra). Tr∆∞·ªõc khi c√≥ data quan s√°t ƒë∆∞·ª£c d·ªØ li·ªáu $\mathcal{D}$, ta tin r·∫±ng $\mathbf{w}$ s·∫Ω t·ªët (l√† m·ªôt m√¥ h√¨nh ph√π h·ª£p v·ªõi $\mathcal{D}$) ·ªü m·ªôt m·ª©c ƒë·ªô n√†o ƒë√≥, t·ª©c l√† $p(\mathbf{w})$, sau khi quan s√°t ƒë∆∞·ª£c d·ªØ li·ªáu ${} \mathcal{D}$ r·ªìi, ni·ªÅm tin v·ªÅ ƒë·ªô ph√π h·ª£p c·ªßa $\mathbf{w}$ v·ªõi ${} \mathcal{D} {}$ s·∫Ω thay ƒë·ªïi v√† c√≥ gi√° tr·ªã l√† $p(\mathbf{w} \mid \mathcal{D})$.

ƒê·ªëi v·ªõi frequentist th√¨ ta s·∫Ω d√πng ph∆∞∆°ng ph√°p **maximum likelihood** (MLE) ƒë·ªÉ t√¨m gi√° tr·ªã $p(\mathcal{D} \mid \mathbf{w})$ l·ªõn nh·∫•t t·ª´ ƒë√≥ t√¨m ƒë∆∞·ª£c $p(\mathbf{w} \mid \mathcal{D})$ l·ªõn nh·∫•t (frequentist gi·∫£ s·ª≠ r·∫±ng $p(\mathbf{w})$ v√† $p(\mathcal{D})$ l√† c√°c h·∫±ng s·ªë, ·ªü g√≥c nh√¨n c·ªßa frequentist, ta s·∫Ω xem $\mathbf{w}$ nh∆∞ l√† m·ªôt gi√° tr·ªã m√† ta ∆∞·ªõc l∆∞·ª£ng ƒë∆∞·ª£c, do ƒë√≥ $p(\mathbf{w})$ l√† m·ªôt h·∫±ng s·ªë) Ta s·∫Ω t√¨m hi·ªÉu ph∆∞∆°ng ph√°p n√†y ·ªü ph·∫ßn [[Zettel/Gaussian Distribution\|Gaussian Distribution]].

C√≤n ƒë·ªëi v·ªõi bayesian, ta c√≥ ph∆∞∆°ng ph√°p g·ªçi l√† **maximum a posteriori estimation** (MAP). Bayesian cho r·∫±ng $\mathbf{w}$ l√† m·ªôt bi·∫øn ng·∫´u nhi√™n ch·ª© kh√¥ng ph·∫£i m·ªôt gi√° tr·ªã, do ƒë√≥ $p(\mathbf{w})$ l√† m·ªôt ph√¢n ph·ªëi. V·∫≠y ƒë·ªÉ t√¨m ƒë∆∞·ª£c $p(\mathbf{w} \mid \mathcal{D})$ l·ªõn nh·∫•t ta ph·∫£i t√¨m c·∫£ likelihood $p(\mathcal{D} \mid \mathbf{w})$ v√† prior $\mathcal{p}(\mathbf{w})$.

---

Ph·∫ßn tr∆∞·ªõc: [[Zettel/Expectations and covariances\|Expectations and covariances]]
Ph·∫ßn sau: [[Zettel/Gaussian Distribution\|Gaussian Distribution]]

---
# References

- [Bishop] Pattern Recognition and Machine Learning - Bishop (chapter 1.2)
- [VHT]  [Machine Learning c∆° b·∫£n (machinelearningcoban.com) - B√†i 31](https://machinelearningcoban.com/2017/07/17/mlemap/)
- [Bayesian Machine Learning (cam.ac.uk)](https://mlg.eng.cam.ac.uk/zoubin/bayesian.html)