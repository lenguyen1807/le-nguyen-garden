---
{"dg-publish":true,"permalink":"/zettel/lam-quen-voi-du-lieu-p2/"}
---


# III. Ph√©p ƒëo ƒë·ªô t∆∞∆°ng ƒë·ªìng v√† kh√°c bi·ªát c·ªßa d·ªØ li·ªáu (TT)

## 8. Jaccard Similarity

- **ƒê·ªô t∆∞∆°ng ƒë·ªìng Jaccard** (jaccard similarity) hay **h·ªá s·ªë Jaccard** (jaccard index) ƒë∆∞·ª£c d√πng ƒë·ªÉ ƒëo s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c *t·∫≠p h·ª£p* v·ªõi nhau.
- X√©t hai t·∫≠p h·ª£p $A$ v√† $B$, khi ƒë√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng Jaccard l√†:
$$
\text{JSim}(A, B) = \dfrac{|A \cap B|}{|A \cup B|} = \dfrac{|A \cap B|}{|A| + |B| - |A \cap B|}
$$
- C√≥ th·ªÉ th·∫•y:
	- $\text{JSim}(A, B) = 0$ n·∫øu $|A \cap B| = 0$, nghƒ©a l√† $A$ v·ªõi $B$ kh√¥ng c√≥ ph·∫ßn t·ª≠ chung.
	- $\text{JSim}(A, B) = 1$ n·∫øu $|A \cap B| = |A \cup B|$, nghƒ©a l√† $A$ v·ªõi $B$ gi·ªëng nhau.
	- Hi·ªÉu m·ªôt nghƒ©a kh√°c, ƒë·ªô t∆∞∆°ng ƒë·ªìng Jaccard ch√≠nh l√† x√°c su·∫•t ch·ªçn ƒë∆∞·ª£c m·ªôt ph·∫ßn t·ª≠ m√† n·∫±m trong c·∫£ $A$ v√† $B$.
	- Gi·∫£ s·ª≠ $A = B = \emptyset$ th√¨ $|A \cup B| = 0$, ƒëi·ªÅu n√†y kh√¥ng th·ªÉ x·∫£y ra ƒë∆∞·ª£c ü•≤, th·∫ø nh∆∞ng theo ƒë·ªãnh nghƒ©a t∆∞∆°ng ƒë·ªìng th√¨ r√µ r√†ng $A$ v·ªõi $B$ gi·ªëng nhau (ƒë·ªÅu l√† $\emptyset$) n√™n ng∆∞·ªùi ta quy ∆∞·ªõc r·∫±ng $\text{JSim}(\emptyset, \emptyset) = 1$.
- Ngo√†i ra ta c√≥ **kho·∫£ng c√°ch Jaccard** l√†:
$$
\text{JDist}(A, B) = 1 - \text{JSim}(A, B) = \dfrac{|A \cup B| - |A \cap B|}{|A \cup B|}
$$
>[!note]
>Ch·ª©ng minh Jaccard Distance l√† m·ªôt metric:
>- [1611.02696.pdf (arxiv.org)](https://arxiv.org/pdf/1612.02696.pdf)
>- [book3.dvi (stanford.edu)](http://infolab.stanford.edu/~ullman/mmds/ch3.pdf)
>- [algorithms - How Jaccard similarity can be approximated with minhash similarity? - Mathematics Stack Exchange](https://math.stackexchange.com/questions/880581/how-jaccard-similarity-can-be-approximated-with-minhash-similarity?rq=1)
>- [elementary set theory - Jaccard dissimilarity and the triangle inequality - Mathematics Stack Exchange](https://math.stackexchange.com/questions/1287118/jaccard-dissimilarity-and-the-triangle-inequality)
>- [inequalities - Is the Jaccard distance a distance? - MathOverflow](https://mathoverflow.net/questions/18084/is-the-jaccard-distance-a-distance)

Gi·∫£ s·ª≠ ta c√≥ 2 ƒë·ªëi t∆∞·ª£ng d·ªØ li·ªáu c√≥ thu·ªôc t√≠nh binary $A$ v√† $B$, n·∫øu xem $A$ v√† $B$ nh∆∞ c√°c t·∫≠p h·ª£p, ta s·∫Ω c√≥ c√°c tr∆∞·ªùng h·ª£p:
- $A \cap B$, nghƒ©a l√† thu·ªôc t√≠nh t·∫°i $A = 0$ v√† $B = 0$ (g·ªçi l√† $q$) ho·∫∑c t·∫°i $A = 1$ v√† t·∫°i $B = 1$ (g·ªçi l√† $t$). Do ƒë√≥ $|A \cap B| = q + t$.
- C√≤n l·∫°i bao g·ªìm $A = 1$ v√† $B = 0$ (g·ªçi l√† $r$) ho·∫∑c $A = 0$ v√† $B = 1$ (g·ªçi l√† $s$).
- Do ƒë√≥ $|A \cup B| = r + s + q + t$.
- Khi ƒë√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng Jaccard gi·ªëng v·ªõi ƒë·ªô ƒëo proximity ·ªü [[Zettel/L√†m quen v·ªõi d·ªØ li·ªáu (P1)#III. Ph√©p ƒëo ƒë·ªô t∆∞∆°ng ƒë·ªìng v√† kh√°c bi·ªát c·ªßa d·ªØ li·ªáu\|L√†m quen v·ªõi d·ªØ li·ªáu (P1)#III. Ph√©p ƒëo ƒë·ªô t∆∞∆°ng ƒë·ªìng v√† kh√°c bi·ªát c·ªßa d·ªØ li·ªáu#3. ƒê·ªô ƒëo proximity cho thu·ªôc t√≠nh binary]]:
$$
\begin{aligned}
\text{JSim}(A, B) &= \dfrac{|A \cap B|}{|A \cup B|} = \dfrac{q + t}{q + r + s + t} \\
\Rightarrow \hspace{5pt} \text{JDist}(A, B) &= 1 - \text{JSim}(A, B) = \dfrac{r + s}{q + r + s + t} = d(A, B)
\end{aligned}
$$

>[!summary]+ T√≥m t·∫Øt
>- D√πng cho t·∫≠p h·ª£p
>- N·∫øu d√πng cho bi·∫øn binary th√¨ t∆∞∆°ng t·ª± nh∆∞ ƒë·ªô ƒëo proximity cho thu·ªôc t√≠nh binary ·ªü [[Zettel/L√†m quen v·ªõi d·ªØ li·ªáu (P1)\|L√†m quen v·ªõi d·ªØ li·ªáu (P1)]]
## 9. Cosine Similarity

- Cho 2 ƒë·ªëi t∆∞·ª£ng d·ªØ li·ªáu $X = (x_1, x_2, ..., x_p)$ v√† $Y = (y_1, y_2, ..., y_p)$ g·ªìm $p$ thu·ªôc t√≠nh. Khi ƒë√≥ **ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine** gi·ªØa $X$ v√† $Y$ l√†:
$$
\text{sim}(X, Y) = \dfrac{X \cdot Y}{||X|| \hspace{3pt} ||Y||}
$$
- Trong ƒë√≥:
	- $X \cdot Y$ l√† t√≠ch v√¥ h∆∞·ªõng c·ªßa $X$ v√† $Y$.
	- $||X||$ l√† ƒë·ªô d√†i c·ªßa $X$ hay l√† $L_2$-norm c·ªßa $X$. T∆∞∆°ng t·ª± v·ªõi $||Y||$.
- N·∫øu ta √°p d·ª•ng l√™n *vƒÉn b·∫£n* th√¨ $X$ v√† $Y$ l√† c√°c *term-frequency* vector c·ªßa hai vƒÉn b·∫£n m√† ta mu·ªën so s√°nh.
- Ngo√†i ra, ta c√≥ **kho·∫£ng c√°ch cosine** l√†:
$$
d(X, Y) = 1 - \text{sim}(X, Y) 
$$
- Th·∫ø nh∆∞ng *kho·∫£ng c√°ch cosine kh√¥ng ph·∫£i l√† 1 metric*, x√©t v√≠ d·ª• ƒë∆°n gi·∫£n sau ƒë√¢y: Gi·∫£ s·ª≠ ta c√≥ 2 vector $a = (1, 2)$ v√† $b = (2, 4)$ khi ƒë√≥ $\text{sim}(a, b) = 1$ n√™n $d(a, b) = 0$ nh∆∞ng kho·∫£ng c√°ch gi·ªØa hai ƒë·ªëi t∆∞·ª£ng kh√°c nhau ph·∫£i lu√¥n $> 0$.

>[!summary]+ T√≥m t·∫Øt
>- D√πng cho thu·ªôc t√≠nh numeric
>- N·∫øu vƒÉn b·∫£n th√¨ t√¨m term-frequency (ho·∫∑c tf-idf) sau ƒë√≥ t√≠nh cosine similarity
## 10. (Pearson) Correlation Coefficient

- Cho hai ƒë·ªëi t∆∞·ª£ng d·ªØ li·ªáu g·ªìm $p$ thu·ªôc t√≠nh $X = (x_1, x_2, ..., x_p)$ v√† $Y = (y_1, y_2, ..., y_p)$ khi ƒë√≥ **h·ªá s·ªë t∆∞∆°ng quan pearson** gi·ªØa $X$ v√† $Y$ l√†:
$$
\text{Corr}(X, Y) = \dfrac{\sum_{i=1}^p(x_i - \overline{X})(y_i - \overline{Y})}{\sqrt{\sum_{i=1}^p(x_i - \overline{X})^2}\sqrt{\sum_{i=1}^p(y_i - \overline{Y})^2}} = \dfrac{\text{Cov}(X, Y)}{\sigma_X\sigma_Y}
$$
- Trong ƒë√≥:
	- $\overline{X}$ l√† mean c·ªßa $X$. T∆∞∆°ng t·ª± v·ªõi $Y$.
	- $\sigma_X$ l√† ƒë·ªô l·ªách chu·∫©n c·ªßa $X$. T∆∞∆°ng t·ª± v·ªõi $Y$.
	- $\text{Cov}(X, Y)$ l√† hi·ªáp ph∆∞∆°ng sai c·ªßa $X$ v√† $Y$.
- Th√¥ng th∆∞·ªùng, ta g·ªçi vector $X'$ l√† vector **chu·∫©n ho√°** c·ªßa $X$ n·∫øu $X' = X - \overline{X} = (x_1 - \overline{X}, ..., x_p - \overline{X})$. Th·∫ø n√™n ·ªü c√¥ng th·ª©c tr√™n, $\text{Corr}(X, Y)$ ch√≠nh l√† ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa c√°c vector chu·∫©n ho√°. 
- ƒê·ªëi v·ªõi c√°c gi√° tr·ªã pearson kh√°c nhau th√¨ ta c√≥ th·ªÉ tr·ª±c quan nh∆∞ sau:

![Pasted image 20240305195836.png](/img/user/Attachment/Pasted%20image%2020240305195836.png)

## 11. Distance cho c√°c thu·ªôc t√≠nh x·∫øp h·∫°ng

Cho 2 rank vector $r_A = (a_1, ..., a_n)$ v√† $r_B = (b_1, ..., b_n)$, khi ƒë√≥ **kho·∫£ng c√°ch spearman** l√†:
$$
d(A, B) = |a_1 - b_1| + |a_2 - b_2| + ... + |a_n - b_n|
$$
Ngo√†i ra, ta c√≥ **kho·∫£ng c√°ch kendal's tau** l√† s·ªë c·∫∑p items c√≥ th·ª© t·ª± kh√°c nhau.

>[!example]+ V√≠ d·ª•
>- Gi·∫£ s·ª≠ ta h·ªèi 2 ng∆∞·ªùi $A$ v√† $B$ v·ªÅ h√£ng ƒë·ªì ƒÉn n√†o ngon h∆°n. Ta c√≥ $A = (McDonald, Jolibee, Lotte)$ v√† $B = (Lotte, McDonald, Jolibee)$ (x·∫øp theo ƒë·ªô ngon tƒÉng d·∫ßn).
>- ƒê·∫ßu ti√™n, ch·ªçn m·ªôt ng∆∞·ªùi l√†m vector ch√≠nh, c√≤n g·ªçi l√† **pattern-vector**, gi·∫£ s·ª≠ ch·ªçn $A$. Do ƒë√≥ ta c√≥ rank vector $r_A = (1, 2, 3)$ v·ªõi $1 = McDonald, 2 = Jolibee$ v√† $3 = Lotte$. Khi ƒë√≥ $r_B = (3, 1, 2)$.
>- Kho·∫£ng c√°ch spearman l√†:
>$$
>d(A, B) = |1 - 3| + |2 - 1| + |3 - 2| = 6
>$$
>- Kho·∫£ng c√°ch kendall = 3 do c√≥ 3 c·∫∑p items kh√°c th·ª© t·ª± bao g·ªìm $(1, 3)$, $(2, 1)$ v√† $(3, 2)$.

>[!warning]
>Theo t√†i li·ªáu t·ª´ ngu·ªìn [Ordinal Distance Tutorial (revoledu.com)](https://people.revoledu.com/kardi/tutorial/Similarity/OrdinalVariables.html) th√¨ kho·∫£ng c√°ch spearman v√† kendell tau kh√°c c√¥ng th·ª©c ph√≠a tr√™n (t·∫°i sao l·∫°i kh√°c th√¨ m√¨nh ch·ªãu). Theo m√¨nh tham kh·∫£o t·ª´ nhi·ªÅu ngu·ªìn (k·ªÉ c·∫£ wiki) th√¨ ƒëa s·ªë kh√°c c√¥ng th·ª©c ph√≠a tr√™n.
>- [Kendall tau distance - Wikipedia](https://en.wikipedia.org/wiki/Kendall_tau_distance)
>- [Spearman Distance Tutorial (revoledu.com)](https://people.revoledu.com/kardi/tutorial/Similarity/SpearmanDistance.html)
>- [Kendall Distance Tutorial (revoledu.com)](https://people.revoledu.com/kardi/tutorial/Similarity/KendallDistance.html)
>
>Trong ƒë√≥:
>- Kho·∫£ng c√°ch spearman:
>$$
>d(A, B) = ||A - B|| = \sum_{i=1}^n (a_i - b_i)^2
>$$
>- Kho·∫£ng c√°ch kendall: L√† s·ªë l·∫ßn ƒë·ªÉ swap ƒë·ªÉ ƒë∆∞a m·ªôt vector v·ªÅ th√†nh pattern vector. N√≥i c√°ch kh√°c l√† s·ªë b∆∞·ªõc c·ªßa "bubble sort" ƒë·ªÉ sort vector ta c·∫ßn x√©t v·ªÅ th√†nh pattern vector.

## 12. Kho·∫£ng c√°ch Hamming

- D√πng cho c√°c vector bit, v√≠ d·ª• nh∆∞ $X = (1, 0, 1, 0, 1, 0)$. **Kho·∫£ng c√°ch Hamming** gi·ªØa hai vector bit $X$ v√† $Y$ l√† s·ªë v·ªã tr√≠ m√† t·∫°i ƒë√≥ bit c·ªßa $X$ v√† $Y$ kh√°c nhau.

>[!example]+ V√≠ d·ª•
>Cho 2 vector bit:
>- $X = (1, 0, 1, 1)$
>- $Y = (1, 1, 0, 1)$
>
>Kho·∫£ng c√°ch Hamming = 2 (kh√°c t·∫°i v·ªã tr√≠ $1$ v√† $2$, v·ªã tr√≠ b·∫Øt ƒë·∫ßu l√† $0$)

- C√≥ th·ªÉ s·ª≠ d·ª•ng kho·∫£ng c√°ch Hamming l√™n thu·ªôc t√≠nh ph√¢n lo·∫°i (nominal, ordinal). Khi ƒë√≥ kho·∫£ng c√°ch hamming gi·ªØa $X$ v√† $Y$ l√† s·ªë v·ªã tr√≠ m√† t·∫°i ƒë√≥ $X$ v√† $Y$ kh√°c gi√° tr·ªã.

>[!example]+ V√≠ d·ª•
>Cho 2 ƒë·ªëi t∆∞·ª£ng d·ªØ li·ªáu sau:
>- $X = (married, sad, rich)$
>- $Y = (single, happy, rich)$
>
>Kho·∫£ng c√°ch Hamming = 2 (kh√°c t·∫°i v·ªã tr√≠ $0$ v√† $1$)

## 13. Kho·∫£ng c√°ch Edit

- ƒê∆∞·ª£c d√πng ƒë·ªÉ ƒëo kho·∫£ng c√°ch gi·ªØa hai chu·ªói.
- Ta c√≥ th·ªÉ hi·ªÉu kho·∫£ng c√°ch edit gi·ªØa chu·ªói $A$ v√† chu·ªói $B$ l√† **s·ªë ph√©p to√°n ƒë·ªÉ bi·∫øn chu·ªói $A$ th√†nh chu·ªói $B$**. 
- C√≥ nhi·ªÅu c√¥ng th·ª©c cho kho·∫£ng c√°ch edit:
	- **Chu·ªói con chung d√†i nh·∫•t** (Longest Common Subsequence): ch·ªâ c√≥ th·ªÉ d√πng hai ph√©p to√°n l√† *xo√°* (delete) v√† *th√™m* (insert) k√Ω t·ª± ƒë·ªÉ bi·∫øn chu·ªói $A$ th√†nh chu·ªói $B$.
	- **Kho·∫£ng c√°ch Levenshtein**: c√≥ th·ªÉ xem nh∆∞ b·∫£n n√¢ng c·∫•p c·ªßa LCS khi cho ph√©p th√™m *thay th·∫ø* (substitution) ƒë·ªÉ thay k√Ω t·ª± n√†y th√†nh k√Ω t·ª± kh√°c (trong slide th√¨ ph√©p thay th·∫ø g·ªçi l√† *ƒë·ªôt bi·∫øn*).

>[!example]+ V√≠ d·ª•
>X√©t 2 chu·ªói $A = kitten$ v√† $B = sitting$. Khi ƒë√≥:
>- Kho·∫£ng c√°ch LCS = 5. Trong ƒë√≥:
>	- Xo√° "k" ƒë·ªÉ $kitten \to itten$.
>	- Th√™m "s" ƒë·ªÉ $itten \to sitten$.
>	- Xo√° "e" ƒë·ªÉ $sitten \to sittn$.
>	- Th√™m "i" ƒë·ªÉ $sittn \to sittin$.
>	- Th√™m "g" ƒë·ªÉ $sittin \to sitting$.
>- Kho·∫£ng c√°ch Levenshtein = 3. Trong ƒë√≥:
>	- Thay "s" th√†nh "k" ƒë·ªÉ $kitten \to sitten$.
>	- Thay "e" th√†nh "i" ƒë·ªÉ $sitten \to sittin$.
>	- Th√™m "g" ƒë·ªÉ $sittin \to sitting$.

- N·∫øu ta xem m·ªôt chu·ªói l√† vector c√°c k√Ω t·ª± th√¨ v·∫´n c√≥ th·ªÉ √°p d·ª•ng kho·∫£ng c√°ch Hamming. Khi ƒë√≥ kho·∫£ng c√°ch Hamming l√† s·ªë v·ªã tr√≠ m√† k√Ω t·ª± c·ªßa hai chu·ªói kh√°c nhau (ch·ªâ √°p d·ª•ng cho c√°c chu·ªói c√πng ƒë·ªô d√†i).

## 14. Kho·∫£ng c√°ch gi·ªØa hai ph√¢n ph·ªëi

X√©t hai ph√¢n ph·ªëi $P$ v√† $Q$ tr√™n kh√¥ng gian m·∫´u $\mathcal{X}$. Ta c√≥ th·ªÉ d√πng nhi·ªÅu c√¥ng th·ª©c ƒë·ªÉ t√¨m kho·∫£ng c√°ch gi·ªØa hai ph√¢n ph·ªëi x√°c su·∫•t.

**Ph√¢n k·ª≥ Kullback-Leibler** (Kullback-Leibler Divergence): K√≠ hi·ªáu l√† $D_{KL}(P \mid \mid Q)$, trong ƒë√≥:
- N·∫øu $P$ v√† $Q$ c√πng l√† *ph√¢n ph·ªëi r·ªùi r·∫°c*:
$$
D_{KL}(P \mid \mid Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \dfrac{P(x)}{Q(x)} \right)
$$
- N·∫øu $P$ v√† $Q$ c√πng l√† *ph√¢n ph·ªëi li√™n t·ª•c*:
$$
D_{KL}(P \mid \mid Q) = \int_{-\infty}^{\infty} P(x) \log \left( \dfrac{P(x)}{Q(x)} \right) dx
$$

>[!note]+
>- M·∫∑c d√π l√† m·ªôt c√¥ng th·ª©c kho·∫£ng c√°ch nh∆∞ng ph√¢n k·ª≥ KL kh√¥ng l√† m·ªôt metric
>- B·ªüi v√¨ ph√¢n k·ª≥ KL kh√¥ng tho·∫£ m√£n t√≠nh ƒë·ªëi x·ª©ng, t·ª©c l√† $d(x, y) \neq d(y, x)$ (https://stats.stackexchange.com/a/188904).

**Ph√¢n k·ª≥ Jensen-Shannon**: K√≠ hi·ªáu l√† $\text{JS}(P \mid \mid Q)$, trong ƒë√≥:
$$
\text{JS}(P \mid \mid Q) = \dfrac{1}{2} D_{KL}(P \mid \mid M) + \dfrac{1}{2} D_{KL}(Q \mid \mid M)
$$
v·ªõi $M = \frac{1}{2}(P + Q)$.

>[!note]
>Gi·ªëng nh∆∞ b·∫£n n√¢ng c·∫•p c·ªßa ph√¢n k·ª≥ KL, ph√¢n k·ª≥ JS tho·∫£ m√£n t√≠nh ƒë·ªëi x·ª©ng.

---

Ph·∫ßn tr∆∞·ªõc: [[Zettel/L√†m quen v·ªõi d·ªØ li·ªáu (P1)\|L√†m quen v·ªõi d·ªØ li·ªáu (P1)]]
Ph·∫ßn sau: [[Zettel/X·ª≠ l√Ω d·ªØ li·ªáu\|X·ª≠ l√Ω d·ªØ li·ªáu]]

---
# References

- [book3.dvi (stanford.edu)](http://infolab.stanford.edu/~ullman/mmds/ch3.pdf)
- [Data Mining. Concepts and Techniques, 3rd Edition (The Morgan Kaufmann Series in Data Management Systems) (sabanciuniv.edu)](https://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf) (ch∆∞∆°ng 2)