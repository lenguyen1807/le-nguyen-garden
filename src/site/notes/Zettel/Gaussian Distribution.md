---
{"dg-publish":true,"permalink":"/zettel/gaussian-distribution/"}
---


**Ph√¢n ph·ªëi chu·∫©n** (Gaussian Distribution ho·∫∑c Normal Distribution), k√≠ hi·ªáu l√† $\mathcal{N}(x \mid \mu, \sigma^2)$, s·∫Ω ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ sau:
$$
\mathcal{N}(x \mid \mu, \sigma^2) = \frac{1}{(2\pi \sigma^2)^{1/2}} \exp \left\{ - \frac{1}{2\sigma^2} (x - \mu)^2 \right\}
$$
V·ªõi $x$ l√† s·ªë th·ª±c v√† $\mathcal{N}(x \mid \mu, \sigma^2)$ c√≥ nghƒ©a l√† ph√¢n ph·ªëi g·ªìm 2 tham s·ªë l√† $\mu$ v√† $\sigma^2$, trong ƒë√≥ $\mu$ l√† **trung b√¨nh** (mean) c·ªßa ph√¢n ph·ªëi, $\sigma^2$ l√† **ph∆∞∆°ng sai** (variance) c·ªßa ph√¢n ph·ªëi.

Ngo√†i ra, n·∫øu l·∫•y cƒÉn c·ªßa ph∆∞∆°ng sai, ta ƒë∆∞·ª£c $\sigma$ v√† ta g·ªçi gi√° tr·ªã ƒë√≥ l√† **ƒë·ªô l·ªách chu·∫©n** (standard derivation) c·ªßa ph√¢n ph·ªëi. C√≤n n·∫øu l·∫•y ngh·ªãch ƒë·∫£o c·ªßa ph∆∞∆°ng sai v√† ƒë·∫∑t gi√° tr·ªã ƒë√≥ l√† $\beta$, t·ª©c l√† $\beta = 1/(\sigma^2)$, ta g·ªçi $\beta$ l√† **ƒë·ªô ch√≠nh x√°c** (precision) c·ªßa ph√¢n ph·ªëi.

>[!note]+
>Khi ta n√≥i m·ªôt bi·∫øn ng·∫´u nhi√™n li√™n t·ª•c $X$ n√†o ƒë√≥ c√≥ ph√¢n ph·ªëi $f$ v·ªõi c√°c tham s·ªë $\theta_{i}$, ta k√≠ hi·ªáu $X \sim f(\theta_{1}, \theta_{2}, \dots)$. V√≠ d·ª•, $X$ c√≥ ph√¢n ph·ªëi chu·∫©n v·ªõi trung b√¨nh l√† $\mu$ v√† ph∆∞∆°ng sai l√† $\sigma^2$ th√¨ ta vi·∫øt $X \sim \mathcal{N}(\mu, \sigma^2)$. Ngo√†i ra khi vi·∫øt $X$ c√≥ ph√¢n ph·ªëi, ta ng·∫ßm hi·ªÉu ph√¢n ph·ªëi ƒë√≥ l√† m·∫≠t ƒë·ªô x√°c su·∫•t (pdf) (ƒë·ªëi v·ªõi bi·∫øn li√™n t·ª•c) c·ªßa $X$.

>[!note]+ 
>[[Zettel/Integral of normal distribution is 1\|Integral of normal distribution is 1]]

![Pasted image 20240613155011.png](/img/user/Attachment/Pasted%20image%2020240613155011.png)
H√¨nh: ƒê·ªì th·ªã c·ªßa ph√¢n ph·ªëi chu·∫©n (ƒë∆∞·ª£c l·∫•y t·ª´ [Bishop]).

Ta c√≥ k√¨ v·ªçng c·ªßa m·ªôt bi·∫øn ng·∫´u nhi√™n $X \sim \mathcal{N}(\mu, \sigma^2)$ l√†:
$$
\mathbb{E}[X] = \int_{-\infty}^{\infty} \mathcal{N}(x \mid \mu, \sigma^2)x dx = \mu
$$
V·∫≠y k√¨ v·ªçng c·ªßa $X$ ch√≠nh l√† gi√° tr·ªã trung b√¨nh c·ªßa ph√¢n ph·ªëi. Ngo√†i ra, ta c√≥:
$$
\mathbb{E}[X^2] = \int_{-\infty}^{\infty} \mathcal{N}(x \mid \mu, \sigma^2)x^2 dx = \mu^2 + \sigma^2
$$
ta g·ªçi gi√° tr·ªã $\mathbb{E}[X^2]$ l√† **moment b·∫≠c 2** (second order moment) c·ªßa $X$. T∆∞∆°ng t·ª± moment b·∫≠c $k$ c·ªßa $X$ s·∫Ω l√† $\mathbb{E}[X^k]$. T·ª´ hai ph∆∞∆°ng tr√¨nh tr√™n, ta c√≥ ph∆∞∆°ng sai c·ªßa $X$ l√†:
$$
\text{var}[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \sigma^2
$$
Gi√° tr·ªã l·ªõn nh·∫•t $x$ l√†m cho ph√¢n ph·ªëi c·ª±c ƒë·∫°i c√≤n ƒë∆∞·ª£c g·ªçi l√† **mode** v√† ph√¢n ph·ªëi chu·∫©n c√≥ $mode = \mu$, t·ª©c l√†:
$$
\text{arg}\max_{x} \mathcal{N}(x \mid \mu, \sigma^2) = \mu
$$
>[!note]+
>Do t√°c gi·∫£ ƒë∆∞a ch·ª©ng minh n√†y v√†o ph·∫ßn b√†i t·∫≠p n√™n m√¨nh c≈©ng s·∫Ω c·ªë g·∫Øng ch·ª©ng minh trong ph·∫ßn b√†i t·∫≠p lu√¥n. Ph·∫ßn b√†i t·∫≠p n·∫±m ·ªü [[Zettel/Exercises Part I\|Exercises Part I]].

X√©t m·ªôt vector $D$ chi·ªÅu g·ªìm c√°c s·ªë th·ª±c $\mathbf{x} =(x_{1}, \dots, x_{D})^T$. Ta ƒë·ªãnh nghƒ©a ph√¢n ph·ªëi chu·∫©n tr√™n vector $\mathbf{x}$ l√†:
$$
\mathcal{N}(\mathbf{x} \mid \pmb{\mu}, \pmb{\Sigma}) = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\pmb{\Sigma}|^{1/2}} \exp \left\{ -\frac{1}{2} (\mathbf{x} - \pmb{\mu})^T \pmb{\Sigma}^{-1} (\mathbf{x} - \pmb{\mu}) \right\}
$$
trong ƒë√≥ $\pmb{\mu}$ l√† vector trung b√¨nh c·ªßa ph√¢n ph·ªëi c√≥ $D$ chi·ªÅu, $\pmb{\Sigma}$ l√† ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai c√≥ k√≠ch th∆∞·ªõc $D \times D$ v√† $|\pmb{\Sigma}|$ l√† ƒë·ªãnh th·ª©c c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai $\pmb{\Sigma}$. M·ªôt t√™n g·ªçi kh√°c cho ph√¢n ph·ªëi chu·∫©n nhi·ªÅu chi·ªÅu l√† **multivariate normal (ho·∫∑c gaussian) distribution**.

>[!danger]+
>M√¨nh kh√¥ng th·ªÉ g√µ ƒë∆∞·ª£c ch·ªØ $x$ nh∆∞ trong s√°ch üò≠. N√™n m√¨nh d√πng k√≠ hi·ªáu l√† $\mathcal{D}$ v·∫≠y.

Gi·∫£ s·ª≠ ta c√≥ m·ªôt t·∫≠p d·ªØ li·ªáu $\mathcal{D} = (x_{1}, \dots, x_{N})^T$. T·∫≠p d·ªØ li·ªáu bao g·ªìm $N$ quan s√°t, m·ªói quan s√°t (observation) l√† m·ªôt ƒë·∫°i l∆∞·ª£ng v√¥ h∆∞·ªõng (scalar) $x_{i}$.

>[!note]+
>Ta g·ªçi m·ªôt gi√° tr·ªã $x$ l√† scalar n·∫øu n√≥ kh√¥ng ph·∫£i l√† vector (ez huh). ƒê√∫ng h∆°n, scalar (hay ƒë·∫°i l∆∞·ª£ng v√¥ h∆∞·ªõng) ƒë·ªÉ ch·ªâ ph·∫ßn t·ª≠ c·ªßa m·ªôt tr∆∞·ªùng (field) ([Scalar (mathematics) - Wikipedia](https://en.wikipedia.org/wiki/Scalar_(mathematics))). T·∫≠p s·ªë th·ª±c ($\mathbb{R}$) l√† m·ªôt tr∆∞·ªùng, do ƒë√≥ ta c√≥ th·ªÉ n√≥i c√°c s·ªë th·ª±c $x \in \mathbb{R}$ l√† m·ªôt ƒë·∫°i l∆∞·ª£ng v√¥ h∆∞·ªõng. Ngo√†i ra, t·∫≠p s·ªë ph·ª©c $\mathbb{C}$ c≈©ng l√† m·ªôt tr∆∞·ªùng n√™n $x$ c≈©ng c√≥ th·ªÉ l√† s·ªë ph·ª©c n·∫øu ta ch·ªâ n√≥i $x$ l√† ƒë·∫°i l∆∞·ª£ng v√¥ h∆∞·ªõng m√† kh√¥ng n√≥i g√¨ th√™m.

Gi·∫£ s·ª≠ c√°c quan s√°t trong t·∫≠p d·ªØ li·ªáu $\mathcal{D}$ c·ªßa ta ƒë∆∞·ª£c l·∫•y ra m·ªôt c√°ch ƒë·ªôc l·∫≠p (drawn independently) t·ª´ m·ªôt ph√¢n ph·ªëi chu·∫©n c√≥ trung b√¨nh l√† $\mu$ v√† ph∆∞∆°ng sai l√† $\sigma^2$ (ƒë√¢y l√† hai ƒë·∫°i l∆∞·ª£ng m√† ta ch∆∞a bi·∫øt v√† m·ª•c ƒë√≠ch c·ªßa ch√∫ng ta l√† t√¨m ra ƒë∆∞·ª£c hai tham s·ªë n√†y t·ª´ t·∫≠p d·ªØ li·ªáu m√† ta c√≥).

>[!note]+
>·ªû c√¢u "l·∫•y ra m·ªôt c√°ch ƒë·ªôc l·∫≠p", ta c√≥ th·ªÉ hi·ªÉu nh∆∞ sau:
>- "L·∫•y ra" (drawn): t·ª©c l√† c√°c gi√° tr·ªã n√†y ƒë∆∞·ª£c ch·ªçn m·ªôt c√°ch ng·∫´u nhi√™n tr√™n ph√¢n ph·ªëi.
>- "ƒê·ªôc l·∫≠p" (independently): khi ta l·∫•y m·ªôt gi√° tr·ªã m·ªõi, c√°c gi√° tr·ªã tr∆∞·ªõc ƒë√≥ (k·ªÉ c·∫£ sau ƒë√≥) kh√¥ng l√†m ·∫£nh h∆∞·ªüng ƒë·∫øn quy·∫øt ƒë·ªãnh ta l·∫•y gi√° tr·ªã m·ªõi n√†o.

C√°c ƒëi·ªÉm d·ªØ li·ªáu m√†:
- ƒê∆∞·ª£c l·∫•y ra t·ª´ c√πng m·ªôt ph√¢n ph·ªëi (identically distributed).
- ƒê·ªôc l·∫≠p v·ªõi nhau (independent).

th√¨ ƒë∆∞·ª£c n√≥i l√† **ƒë·ªôc l·∫≠p v√† c√≥ ph√¢n ph·ªëi ƒë·ªìng nh·∫•t** (independent and identically distributed) v√† th∆∞·ªùng vi·∫øt t·∫Øt l√† i.i.d.

>[!note]+
>X√©t t·∫≠p d·ªØ li·ªáu $\mathcal{D}$, n·∫øu ta vi·∫øt $\mathcal{D} \overset{i.i.d}{\sim} \mathcal{N}(\mu, \sigma^2)$ t·ª©c l√† t·∫≠p d·ªØ li·ªáu $\mathcal{D}$ l√† ƒë·ªôc l·∫≠p v√† c√≥ ph√¢n ph·ªëi ƒë·ªìng nh·∫•t, ngo√†i ra $\mathcal{D}$ ƒë∆∞·ª£c l·∫•y ra t·ª´ ph√¢n ph·ªëi chu·∫©n.

X√©t h√†m likelihood $\mathcal{L}(\mu, \sigma^2 \mid \mathcal{D})$, b·ªüi v√¨ $\mathcal{D}$ l√† i.i.d n√™n ta c√≥:
$$
\mathcal{L}(\mu, \sigma^2 \mid \mathcal{D}) = p(\mathcal{D} \mid \mu, \sigma^2) = p(x_{1}, \dots, x_{N} \mid \mu, \sigma^2) = \prod_{n=1}^N p(x_{n} \mid \mu, \sigma^2)
$$
>[!note]+
>Nh∆∞ ta ƒë√£ n√≥i ·ªü ph·∫ßn tr∆∞·ªõc ([[Zettel/Introduction (Prob)\|Introduction (Prob)]]), khi $X$ v√† $Y$ ƒë·ªôc l·∫≠p v·ªõi nhau th√¨:
>$$
p(X, Y) = p(X)p(Y)
>$$

M·ªôt trong nh·ªØng c√°ch th∆∞·ªùng d√πng ƒë·ªÉ t√¨m c√°c tham s·ªë cho ph√¢n ph·ªëi b·∫±ng c√°ch s·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu quan s√°t ƒë∆∞·ª£c ($\mathcal{D}$) l√† t√¨m c√°c tham s·ªë m√† **l√†m c·ª±c ƒë·∫°i** h√†m likelihood (hay c√≤n g·ªçi l√† **maximum likelihood**). Hay n√≥i c√°ch kh√°c:
$$
\hat{\mu}, \hat{\sigma}^2 = \text{arg}\max_{\mu, \sigma^2} \mathcal{L}(\mu, \sigma^2 \mid \mathcal{D})
$$
>[!note]+
>K√≠ hi·ªáu $\displaystyle \text{arg}\max_{x} f(x)$ c√≥ nghƒ©a l√† gi√° tr·ªã $x$ sao cho $f(x)$ l√† l·ªõn nh·∫•t (c·ª±c ƒë·∫°i).

ƒê·ªÖ d·ªÖ d√†ng h∆°n, thay v√¨ t√¨m c√°c tham s·ªë l√†m c·ª±c ƒë·∫°i h√†m likelihood, ta t√¨m c√°c tham s·ªë l√†m c·ª±c ƒë·∫°i h√†m log (log ·ªü ƒë√¢y s·∫Ω ƒë∆∞·ª£c hi·ªÉu l√† $\ln$) c·ªßa h√†m likelihood, nghƒ©a l√†:
$$
\hat{\mu}, \hat{\sigma}^2 = \text{arg}\max_{\mu, \sigma^2} \ln \mathcal{L}(\mu, \sigma^2 \mid \mathcal{D})
$$
S·ª≠ d·ª•ng c√°ch th·ª©c c·ª±c ƒë·∫°i h√†m log likehood, ta c√≥ th·ªÉ vi·∫øt h√†m likelihood l·∫°i nh∆∞ sau:
$$
\begin{aligned}
\ln \mathcal{L}(\mu, \sigma^2 \mid \mathcal{D}) &= \ln \prod_{n=1}^N p(x_{n} \mid \mu, \sigma^2) \\
&= \left[-\frac{1}{2\sigma^2} \sum_{n=1}^N (x_{n} - \mu)^2 \right] - \frac{N}{2} \ln 2\pi - \frac{N}{2}\ln \sigma^2
\end{aligned}
$$
C·ª±c ƒë·∫°i h√†m log likelihood ph√≠a tr√™n b·∫±ng c√°ch d√πng $\mu$, ta c√≥:
$$
\mu_{ML} = \frac{1}{N} \sum_{n=1}^N x_{n}
$$
trong ƒë√≥ $\mu_{ML}$ ƒë∆∞·ª£c g·ªçi l√† **trung b√¨nh m·∫´u** (sample mean) t·ª©c l√† trung b√¨nh c·ªßa c√°c quan s√°t $\{x_n\}$ m√† ta quan s√°t ƒë∆∞·ª£c. C√≤n n·∫øu ta c·ª±c ƒë·∫°i b·∫±ng c√°ch d√πng $\sigma^2$, ta c√≥:
$$
\sigma^2_{ML} = \frac{1}{N} \sum_{n=1}^N (x_{n} - \mu_{ML})^2
$$
ta g·ªçi $\sigma^2_{ML}$ l√† **ph∆∞∆°ng sai m·∫´u** (sample variance), ngo√†i ra ta th·∫•y $\sigma^2_{ML}$ c≈©ng ph·ª• thu·ªôc v√†o $\mu_{ML}$. V·ªÅ l√Ω thuy·∫øt l√† ta c·∫ßn t√≠nh c·∫£ hai c√πng l√∫c (t√¨m b·ªô tham s·ªë l√†m c·ª±c ƒë·∫°i, m√† b·ªô tham s·ªë g·ªìm $n$ bi·∫øn th√¨ t√¨m c√πng l√∫c $n$ bi·∫øn) th·∫ø nh∆∞ng trong tr∆∞·ªùng h·ª£p n√†y, $\mu_{ML}$ kh√¥ng ph·ª• thu·ªôc v√†o $\sigma^2_{ML}$ do ƒë√≥ ta c√≥ th·ªÉ t√¨m $\mu_{ML}$ tr∆∞·ªõc sau ƒë√≥ t√¨m $\sigma^2_{ML}$.

>[!note]+ Ch·ª©ng minh
>[[Zettel/Maximum Log likelihood\|Maximum Log likelihood]]

>[!note]+
>Th√¥ng th∆∞·ªùng gi√° tr·ªã trung b√¨nh $\mu$ ƒë∆∞·ª£c g·ªçi trung b√¨nh t·ªïng th·ªÉ (population mean) t∆∞∆°ng t·ª± v·ªõi $\sigma^2$ l√† ph∆∞∆°ng sai t·ªïng th·ªÉ (variance mean), ƒë√¢y l√† gi√° tr·ªã m√† ta kh√¥ng bi·∫øt, th·∫ø nh∆∞ng b·∫±ng c√°ch d√πng m·ªôt ph·∫ßn c·ªßa t·ªïng th·ªÉ (g·ªçi l√† m·∫´u), ta s·∫Ω c·ªë g·∫Øng ∆∞·ªõc l∆∞·ª£ng ƒë∆∞·ª£c gi√° tr·ªã $\mu$ v·ªõi $\sigma^2$ t·ªët nh·∫•t. Nh∆∞ ƒë√£ ch·ª©ng minh ph√≠a tr√™n, gi√° tr·ªã ∆∞·ªõc l∆∞·ª£ng t·ªët nh·∫•t ch√≠nh l√† $\mu_{ML}$ (trung b√¨nh c·ªßa m·∫´u) v√† $\sigma^2_{ML}$ (ph∆∞∆°ng sai c·ªßa m·∫´u).

X√©t gi√° tr·ªã k√¨ v·ªçng c·ªßa trung b√¨nh m·∫´u $\mu_{ML}$, ta c√≥:
$$
\mathbb{E}[\mu_{ML}] = \mu
$$
c√≥ th·ªÉ th·∫•y, k√¨ v·ªçng c·ªßa trung b√¨nh m·∫´u $\mu_{ML}$ ch√≠nh l√† trung b√¨nh c·ªßa ph√¢n ph·ªëi $\mu$, ƒë√∫ng nh∆∞ ta d·ª± ƒëo√°n, gi√° tr·ªã $\mu_{ML}$ c√≥ th·ªÉ ƒë∆∞·ª£c d√πng ∆∞·ªõc l∆∞·ª£ng r·∫•t t·ªët $\mu$. Th·∫ø nh∆∞ng, n·∫øu x√©t gi√° tr·ªã k√¨ v·ªçng c·ªßa ph∆∞∆°ng sai m·∫´u $\sigma^2_{ML}$, ta c√≥:
$$
\mathbb{E}[\sigma^2_{ML}] = \frac{(N-1)}{N} \sigma^2
$$
M·∫∑c d√π ∆∞·ªõc l∆∞·ª£ng t·ªët v·ªõi $\mu_{ML}$ th·∫ø nh∆∞ng $\sigma^2_{ML}$ th√¨ kh√¥ng. D√πng $\sigma^2_{ML}$ ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng cho $\sigma^2$ th√¨ cho ra gi√° tr·ªã th·∫•p h∆°n, ta g·ªçi c√°ch ∆∞·ªõc l∆∞·ª£ng n√†y l√† **ƒë√°nh gi√° th·∫•p** (underestimate) (hay c√≤n g·ªçi l√† bias). ƒê·ªÉ tr√°nh vi·ªác bias nh∆∞ n√†y, ta ch·ªâ c·∫ßn chia cho $N-1$ thay v√¨ $N$ ·ªü ph∆∞∆°ng sai m·∫´u:
$$
\begin{aligned}
\sigma^2_{ML} &= \frac{1}{N-1} \sum_{n=1}^N (x_{n} - \mu_{ML})^2 \\
\implies \mathbb{E}[\sigma^2_{ML}] &= \sigma^2
\end{aligned}
$$
v√† ƒë√¢y l√† l√Ω do m√† ng∆∞·ªùi ta th∆∞·ªùng chia cho $N-1$ thay v√¨ $N$ ·ªü ph∆∞∆°ng sai m·∫´u.

>[!note]+ Ch·ª©ng minh
>[[Zettel/Expected values of sample mean and sample variance\|Expected values of sample mean and sample variance]]

Th·∫ø nh∆∞ng khi $N$ tr·ªü l√™n l·ªõn d·∫ßn, vi·ªác bias c·ªßa nghi·ªám c·ªßa maximum likelihood ($\sigma^2_{ML}$) kh√¥ng c√≤n qu√° quan tr·ªçng n·ªØa (v√≠ d·ª• b·∫°n c√≥ $N = 100001$ th√¨ $N - 1 = 100000$ s·∫Ω cho ra k·∫øt qu·∫£ kh√¥ng qu√° ch√™nh l·ªách). Khi m√† $N \to \infty$ th√¨ ph∆∞∆°ng sai m·∫´u $\sigma^2_{ML}$ s·∫Ω ti·∫øn d·∫ßn v·ªÅ ph∆∞∆°ng sai th·ª±c s·ª± $\sigma$ c·ªßa ph√¢n ph·ªëi. Trong th·ª±c t·∫ø, n·∫øu $N$ kh√¥ng nh·ªè th√¨ bias kh√¥ng ph·∫£i l√† m·ªôt v·∫•n ƒë·ªÅ quan tr·ªçng l·∫Øm.

>[!note]+
>Vi·ªác ph∆∞∆°ng sai m·∫´u $\sigma^2_{ML}$ ti·∫øn d·∫ßn v·ªÅ ph∆∞∆°ng sai th·ª±c s·ª± $\sigma$ c·ªßa ph√¢n ph·ªëi khi m√† $N \to \infty$ ƒë∆∞·ª£c ch·ª©ng minh c·ª• th·ªÉ ·ªü **lu·∫≠t s·ªë l·ªõn** (Law of Large Number).
>[Law of large numbers - Wikipedia](https://en.wikipedia.org/wiki/Law_of_large_numbers)
>[probability theory - Sample variance converge almost surely - Mathematics Stack Exchange](https://math.stackexchange.com/questions/243348/sample-variance-converge-almost-surely)

Tuy nhi√™n v·ªõi c√°c m√¥ h√¨nh ML ph·ª©c t·∫°p c√≥ nhi·ªÅu tham s·ªë th√¨ v·∫•n ƒë·ªÅ bias n√†y l·∫°i tr·ªü n√™n nghi√™m tr·ªçng. ·ªû c√°c ph·∫ßn sau, t√°c gi·∫£ s·∫Ω cho th·∫•y v·∫•n ƒë·ªÅ bias c·ªßa maximum likelihood l√† m·ªôt trong nh·ªØng nguy√™n nh√¢n g√¢y ra over-fitting.

---

Ph·∫ßn tr∆∞·ªõc: [[Zettel/Bayesian Probabilities\|Bayesian Probabilities]]
Ph·∫ßn sau: [[Zettel/Curve Fitting Revisited\|Curve Fitting Revisited]]

---
# References

- [Bishop] Pattern Recognition and Machine Learning - Bishop (chapter 1.2)
- [Understanding Moments (gregorygundersen.com)](https://gregorygundersen.com/blog/2020/04/11/moments/)