---
{"dg-publish":true,"permalink":"/zettel/polynomial-curve-fitting/"}
---


Gi·∫£ s·ª≠ b·∫°n LN c√≥ s·ªë ti·ªÅn l√† $x \in \mathbb{R}$ v√† LN mu·ªën d·ª± ƒëo√°n xem v·ªõi s·ªë ti·ªÅn $x$ n√†y, LN c√≥ th·ªÉ mua ƒë∆∞·ª£c bao nhi√™u c√°i b√°nh x√®o, b·ªüi v√¨ b√† b√°n b√°nh x√®o b·∫£ kh√¥ng mu·ªën ti·∫øt l·ªô gi√° 1 c√°i b√°nh x√®o v√† gi√° c√≥ th·ªÉ m·ªói ng√†y thay ƒë·ªïi (nh∆∞ng b√°nh x√®o ngon).

G·ªçi s·ªë b√°nh x√®o mua ƒë∆∞·ª£c l√† $y \in \mathbb{R}$ (trong th·ª±c t·∫ø s·ªë b√°nh s·∫Ω l√† s·ªë nguy√™n, nh∆∞ng m√† m√¨nh gi·∫£ s·ª≠ b√† b√°n b√°nh c√≥ kh·∫£ nƒÉng n√†o ƒë√≥ b√°n ƒë∆∞·ª£c s·ªë b√°nh x√®o l√† s·ªë th·ª±c).

>[!important]
>B√†i to√°n d√πng m·ªôt gi√° tr·ªã ƒë·∫ßu v√†o $x$ ƒë·ªÉ d·ª± ƒëo√°n m·ªôt gi√° tr·ªã s·ªë th·ª±c $y$ n√†o ƒë√≥ ƒë∆∞·ª£c g·ªçi l√† b√†i to√°n **h·ªìi quy** (regression).

Gi·∫£ s·ª≠ LN thu th·∫≠p d·ªØ li·ªáu cho vi·ªác d·ª± ƒëo√°n c·ªßa m√¨nh th√¥ng qua $N$ l·∫ßn mua, ƒë·∫∑t l√† $\mathbf{x} \equiv (x_1, ..., x_N)^T$. V·ªõi m·ªói $x_i$, LN bi·∫øt ƒë∆∞·ª£c s·ªë b√°nh x√®o mua ƒë∆∞·ª£c l√† $y_i$, v·∫≠y ta c√≥ $\mathbf{y} \equiv (y_1, ..., y_N)^T$. Ta g·ªçi d·ªØ li·ªáu m√† LN thu th·∫≠p ƒë∆∞·ª£c $\{ (x_1, y_1), ..., (x_N, y_N) \}$ l√† **t·∫≠p hu·∫•n luy·ªán** (training set). D·ª±a tr√™n t·∫≠p hu·∫•n luy·ªán ·∫•y, LN mu·ªën d·ª± ƒëo√°n gi√° tr·ªã $\hat{y}$ cho m·ªôt gi√° tr·ªã $\hat{x}$ m·ªõi (l·∫ßn mua m·ªõi). V√¨ v·∫≠y LN c·∫ßn t√¨m ra m·ªôt m√¥ h√¨nh n√†o ƒë√≥, ƒë·ªÉ khi ƒë∆∞a v√†o input $\hat{x}$, m√¥ h√¨nh cho ra ƒë∆∞·ª£c gi√° tr·ªã $\hat{y}$ ph√π h·ª£p.

![Pasted image 20240415140535.png](/img/user/Attachment/Pasted%20image%2020240415140535.png)
H√¨nh: D·ªØ li·ªáu m√† LN thu th·∫≠p ƒë∆∞·ª£c sau $N = 10$ l·∫ßn mua

>[!note]
>ƒê∆∞·ªùng cong m√†u xanh l√° t∆∞∆°ng ·ª©ng v·ªõi h√†m $f(x) = \sin(2\pi x) + 2$, m√¨nh d√πng h√†m n√†y ƒë·ªÉ sinh ra data, sau ƒë√≥ c·ªông th√™m v·ªõi gi√° tr·ªã random ƒë∆∞·ª£c l·∫•y t·ª´ ph√¢n ph·ªëi chu·∫©n v·ªõi $\mu = 0$ v√† $\sigma = 0.3$ (m·ª•c ƒë√≠ch l√† ƒë·ªÉ t·∫°o ra noise cho gi·ªëng v·ªõi data th·ª±c t·∫ø) cho ra c√°c gi√° tr·ªã $y$ m√†u ƒë·ªè (theo [Bishop]). M·ª•c ƒë√≠ch c·ªßa ch√∫ng ta l√† d·ª±a tr√™n data ƒë·ªÉ t√¨m ra h√†m $f$ t·ªët nh·∫•t, th·∫ø nh∆∞ng data th√¥ng th∆∞·ªùng s·∫Ω c√≥ r·∫•t nhi·ªÅu noise v√† h√†m $f$ trong th·ª±c t·∫ø r·∫•t kh√≥ t√¨m.

LN ƒëo√°n r·∫±ng, vi·ªác d√πng m·ªôt h√†m ƒëa th·ª©c nh∆∞ d∆∞·ªõi c√≥ th·ªÉ d·ª± ƒëo√°n ƒë∆∞·ª£c s·ªë b√°nh:
$$
f(x, \mathbf{w}) = w_{0} + w_{1}x + w_{2}x^2 + \dots w_{M}x^M = \sum_{i=0}^M w_{i}x^i
$$
trong ƒë√≥ $\mathbf{w} = (w_0, w_1, ..., w_M)^T$ l√† b·ªô tham s·ªë c·ªßa h√†m ƒëa th·ª©c $f(x, \mathbf{w})$ m√† LN ch·ªçn v√† $M$ l√† *b·∫≠c* c·ªßa ƒëa th·ª©c ·∫•y. V·∫≠y c√≥ hai th·ª© m√† ta c·∫ßn ƒë·ªÉ √Ω khi d√πng ƒëa th·ª©c (hay m√¥ h√¨nh) n√†y, ƒë√≥ ch√≠nh l√† $M$ v√† $\mathbf{w}$, ƒë·ªÉ t√¨m ƒë∆∞·ª£c m√¥ h√¨nh ph√π h·ª£p v·ªõi data ƒë√£ c√≥, ta c·∫ßn t√¨m $\mathbf{w}$ v√† $M$ ph√π h·ª£p.

>[!note]
>M·∫∑c d√π $f(x, \mathbf{w})$ l√† m·ªôt h√†m kh√¥ng tuy·∫øn t√≠nh c·ªßa $x$ nh∆∞ng l·∫°i l√† m·ªôt h√†m tuy·∫øn t√≠nh c·ªßa $\mathbf{w}$, v√¨ v·∫≠y ta c√≥ th·ªÉ xem b√†i to√°n n√†y l√† m·ªôt b√†i to√°n **h·ªìi quy tuy·∫øn t√≠nh** (linear regression) b·ªüi v√¨ ƒë√¢y l√† b√†i to√°n regression v√† ta d√πng m√¥ h√¨nh linear ü§Ø.

ƒê·ªÉ t√¨m ƒë∆∞·ª£c $\mathbf{w}$ ph√π h·ª£p v·ªõi d·ªØ li·ªáu, ta c·∫ßn m·ªôt h√†m ƒë·ªÉ ƒë√°nh gi√° xem, li·ªáu gi√° tr·ªã $\mathbf{w}$ m√† ta ch·ªçn t·ªët hay l√† kh√¥ng (kh√¥ng t·ªët th√¨ ch·ªçn l·∫°i, ch·ªçn n√†o t·ªët th√¨ th√¥i üëå). H√†m ƒë√°nh gi√° ƒë√≥ ƒë∆∞·ª£c g·ªçi l√† **h√†m l·ªói** (error function), k√≠ hi·ªáu $E(\mathbf{w})$, $E(\mathbf{w})$ c√†ng th·∫•p (l·ªói c√†ng √≠t) th√¨ $\mathbf{w}$ c√†ng ph√π h·ª£p v·ªõi d·ªØ li·ªáu, do ƒë√≥ m·ª•c ti√™u t·ªëi th∆∞·ª£ng üò§ l√† ph·∫£i t√¨m gi√° tr·ªã $\mathbf{w}^\star$ sao cho $E(\mathbf{w}^\star)$ l√† nh·ªè nh·∫•t.

V·ªõi m·ªói lo·∫°i b√†i to√°n kh√°c nhau th√¨ ta s·∫Ω ch·ªçn error function kh√°c nhau. ƒê·ªÉ cho d·ªÖ, m√¨nh s·∫Ω ch·ªçn h√†m l·ªói nh∆∞ sau:
$$
E(\mathbf{w}) = \frac{1}{2} \sum_{i=1}^N [f(x_{i}, \mathbf{w}) - y_{i}]^2
$$
ta c√≥ th·ªÉ hi·ªÉu h√†m l·ªói n√†y l√† t·ªïng b√¨nh ph∆∞∆°ng ƒë·ªô l·ªói (hay ƒë·ªô kh√°c nhau) gi·ªØa s·ªë b√°nh d·ª± ƒëo√°n $f(x_i, \mathbf{w})$ v√† s·ªë b√°nh th·ª±c s·ª± mua ƒë∆∞·ª£c $y_i$. Ngo√†i ra $E(\mathbf{w}) = 0$ khi v√† ch·ªâ khi $f(x_{i}, \mathbf{w}) = y_{i} \hspace{3pt} \forall i = 1\dots N$, nghƒ©a l√† kh√¥ng c√≥ l·ªói n√†o ·ªü ƒë√¢y, $f(x, \mathbf{w})$ kh·ªõp ho√†n to√†n v·ªõi d·ªØ li·ªáu.

>[!note]+
>V·ªõi h√†m l·ªói nh∆∞ tr√™n, ta ho√†n to√†n c√≥ th·ªÉ t√¨m ƒë∆∞·ª£c gi√° tr·ªã $\mathbf{w}^\star$ b·ªüi v√¨ $f(x_i, \mathbf{w})$ l√† m·ªôt h√†m linear n√™n $[f(x_{i}, \mathbf{w}) - y_{i}]^2$ l√† m·ªôt h√†m b·∫≠c 2, do ƒë√≥ ƒë·∫°o h√†m c·ªßa $E(\mathbf{w})$ theo $w_i$ l√† m·ªôt h√†m linear, do ƒë√≥ t·ªìn t·∫°i m·ªôt nghi·ªám duy nh·∫•t.
>$$
\dfrac{\partial E(\mathbf{w})}{\partial \mathbf{w}} = \begin{bmatrix}
\dfrac{\partial{E(\mathbf{w})}}{\partial w_{0}} \dots 
\dfrac{\partial{E(\mathbf{w})}}{\partial w_{N}}
\end{bmatrix}^T
>$$

V·∫•n ƒë·ªÅ t√¨m theo ƒë√≥ ch√≠nh l√† t√¨m gi√° tr·ªã $M$ ph√π h·ª£p. 

---

Ph·∫ßn sau: [[Zettel/Introduction (Prob)\|Introduction (Prob)]]

---
# References

- [Bishop] Pattern Recognition and Machine Learning - Bishop (chapter 1.1).