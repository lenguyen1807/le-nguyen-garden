---
{"dg-publish":true,"permalink":"/zettel/poisson-distribution-and-stirling-approximation/","noteIcon":"üìù","created":"2024-06-30T19:11:48.454+07:00","updated":"2024-07-12T08:42:41.479+07:00"}
---


X√©t ph√¢n ph·ªëi $X$ nh∆∞ d∆∞·ªõi ƒë√¢y:
$$
P(X = r \mid \lambda) = e^{-\lambda} \dfrac{\lambda^r}{r!} \hspace{5pt} \text{v·ªõi $r \in \{0, 1, 2, \dots\}$}.
$$
ta g·ªçi ph√¢n ph·ªëi tr√™n l√† **ph√¢n ph·ªëi Poisson**.

>[!note]+
>Ph√¢n ph·ªëi chu·∫©n hay c√≤n g·ªçi l√† ph√¢n ph·ªëi gaussian c√≥ c√¥ng th·ª©c nh∆∞ sau:
>$$
>P(X = x \mid \mu, \sigma^2) = \dfrac{1}{\sigma\sqrt{2 \pi}} e^{\dfrac{-(x - \mu)^2}{2\sigma^2}}.
>$$
>trong ƒë√≥ $\mu$ l√† trung b√¨nh c·ªßa $X$, $\sigma^2$ l√† ph∆∞∆°ng sai c·ªßa $X$ v√† $\sigma$ l√† ƒë·ªô l·ªách chu·∫©n c·ªßa $X$.

Khi $\lambda$ ƒë·ªß l·ªõn (th√¥ng th∆∞·ªùng l√† $\lambda > 1000$), ta c√≥ th·ªÉ x·∫•p x·ªâ ph√¢n ph·ªëi Poisson v·ªõi ph√¢n ph·ªëi chu·∫©n c√≥ $\mu = \lambda$ v√† $\sigma^2 = \lambda \implies \sigma = \sqrt{\lambda}$ (t√¨m hi·ªÉu th√™m ·ªü [ƒë√¢y](https://stats.stackexchange.com/questions/83283/normal-approximation-to-the-poisson-distribution?rq=1)), v·∫≠y:
$$
e^{-\lambda} \dfrac{\lambda^r}{r!} \simeq\dfrac{1}{\sqrt{2\pi \lambda}} e^{\dfrac{-(r - \lambda)^2}{2\lambda}}.
$$
X√©t ph∆∞∆°ng tr√¨nh x·∫•p x·ªâ ph√≠a tr√™n, n·∫øu thay $r = \lambda$, ta ƒë∆∞·ª£c:
$$
\begin{aligned}
e^{-\lambda} \dfrac{\lambda^\lambda}{\lambda!} &\simeq \dfrac{1}{\sqrt{2\pi \lambda}} e^{\dfrac{-(\lambda - \lambda)^2}{2\lambda}} \\
\Leftrightarrow e^{-\lambda} \dfrac{\lambda^\lambda}{\lambda!} &\simeq \dfrac{1}{\sqrt{2\pi \lambda}} \\
\Leftrightarrow \lambda! &\simeq \lambda^{\lambda} e^{-\lambda} \sqrt{2\pi\lambda}.
\end{aligned}
$$
·ªû ph∆∞∆°ng tr√¨nh x·∫•p x·ªâ tr√™n, ta c√≥ th·ªÉ th·∫•y giai th·ª´a c·ªßa m·ªôt s·ªë ƒë∆∞·ª£c x·∫•p x·ªâ b·∫±ng m·ªôt c√¥ng th·ª©c kh√°c, ta g·ªçi c√¥ng th·ª©c ƒë√≥ l√† **x·∫•p x·ªâ Stirling** ([Stirling's approximation](https://en.wikipedia.org/wiki/Stirling%27s_approximation)). N·∫øu ta l·∫•y $\ln$ c·ªßa x·∫•p x·ªâ stirling, ta ƒë∆∞·ª£c:
$$
\begin{aligned}
\ln x! &\simeq \ln x^xe^{-x}\sqrt{2\pi x} \\
&\simeq \ln x^x  + \ln e^{-x} + \ln \sqrt{ 2\pi x } \\
&\simeq x\ln x - x + \frac{1}{2}\ln 2 \pi x.
\end{aligned}
$$
>[!note]+
>Ngo√†i c√°ch vi·∫øt nh∆∞ tr√™n, ng∆∞·ªùi ta c≈©ng th∆∞·ªùng vi·∫øt nh∆∞ sau:
>$$
>x\ln x - x + O(\ln x).
>$$

Gi√° tr·ªã n·∫±m ph√≠a cu·ªëi $\frac{1}{2} \ln 2\pi x$ ƒë∆∞·ª£c g·ªçi l√† *next-order correction term* (m√¨nh c≈©ng kh√¥ng r√µ ƒëo·∫°n n√†y), ta ho√†n to√†n c√≥ th·ªÉ b·ªè n√≥ v√† s·ª≠ d·ª•ng:
$$
x! \simeq x\ln x - x.
$$
Ta bi·∫øt r·∫±ng:
$$
{N \choose r} = \frac{N!}{(N - r)! r!}.
$$
Do ƒë√≥, ta ho√†n to√†n c√≥ th·ªÉ x·∫•p x·ªâ $N \choose r$ b·∫±ng c√°ch d√πng x·∫•p x·ªâ Stirling:
$$
\begin{aligned}
\ln \frac{N!}{(N - r)! r!} &= \ln N! - \ln (N-r)! - \ln r \\
&\simeq N\ln N - N - (N-r)\ln(N-r) + (N-r) - r\ln r + r \\
&= N\ln N - (N-r)\ln(N-r) - r\ln r \\
&= (N-r)\ln N - (N-r)\ln(N-r) + r\ln N - r\ln r\\
&= (N-r)\ln \frac{N}{N-r} + r\ln \frac{N}{r}.
\end{aligned}
$$
Ta bi·∫øt r·∫±ng:
$$
\log_{2}x = \frac{\log_{e}x}{\log_{e}2} = \frac{\ln x}{\ln 2}.
$$
t·ª©c l√† gi·ªØa $\ln x$ v√† $\log_{2} x$ (n·∫øu ch·ªâ vi·∫øt $\log$ th√¨ ta hi·ªÉu ƒë√¢y l√† $\log_{2}$) ch·ªâ kh√°c nhau m·ªôt h·∫±ng s·ªë, do ƒë√≥ vi·ªác x·∫•p x·ªâ $\ln x!$ c√≥ th·ªÉ thay th·∫ø th√†nh $\log x!$ m√† kh√¥ng c·∫ßn thay ƒë·ªïi c√¥ng th·ª©c, ch·ªâ th√™m h·∫±ng s·ªë v√†o, do ƒë√≥ c√≥ th·ªÉ b·ªè lu√¥n h·∫±ng s·ªë ƒë√≥ trong khi x·∫•p x·ªâ.
$$
\log{N\choose r} \simeq (N-r)\log \frac{N}{N-r} + r\log \frac{N}{r}.
$$
---
# References

- [MacKay] Information Theory, Inference and Learning Algorithms - MacKay (chapter 1)